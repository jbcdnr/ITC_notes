\documentclass{article}
% General document formatting
\usepackage[margin=1in]{geometry}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{amsthm}
\usepackage{cleveref}
\usepackage{xcolor}

\title{Information Theory and Coding - Prof.~Emere Telatar}
\date{\today}
\author{Jean-Baptiste Cordonnier, Sebastien Speierer, Thomas Batschelet}

% define example environments
\newcounter{example}[section]
\newenvironment{example}
    {
    \refstepcounter{example}
    \begin{center}
	    \begin{tabular}{|p{0.8\textwidth}|}
		    \hline\\
			\textbf{Example~\theexample.}\\
		    }
		    {
		    \\\hline
	    \end{tabular}
    \end{center}
    }

% define definition environments
% \newcounter{definition}[section]
% \newenvironment{definition}
%     {
%     \refstepcounter{definition}
%     \begin{center}
% 	    \begin{tabular}{|p{0.8\textwidth}|}
% 		    \hline\\
% 			\textbf{Definition~\thedefinition.}\\
% 		    }
% 		    {
% 		    \\\hline
% 	    \end{tabular}
%     \end{center}
%     }

\newtheorem{theorem}{Theorem}[section]
\newtheorem{corollary}{Corollary}[theorem]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{definition}{Definition}[section]

\newcommand{\todo}[1]{\textcolor{red}{TODO: #1}}

% math bold
\def\*#1{\mathbf{#1}}
% manuscript characters
\def\D{\mathcal{D}}
\def\V{\mathcal{V}}
\def\L{\mathcal{L}}
\def\U{\mathcal{U}}
\def\N{\mathcal{N}}
\def\X{\mathcal{X}}
\def\C{\mathcal{C}}


\begin{document}

\maketitle


\section{Data compression}

Given an alphabet $\U$ (e.g. $\U = \{a, ..., z, A, ..., Z, ...\}$), we want to assign binary sequences to elements of $\U$, i.e.

\begin{align*}
	e: \U \rightarrow {0, 1}^* = \{\emptyset, 0, 1, 00, 01, ...\}
\end{align*}

For $\X$ a set

\begin{align*}
	\X^n &\equiv \{ (x_0 ... x_n), x_i \in \X\} \\
	\X^* &\equiv \bigcup_{n \geq 0} \X^n
\end{align*}

\begin{definition}
	A code $\C$ is called \textit{singular} if
	\begin{align*}
		\exists (u, v) \in \U^2, u \neq v \quad s.t. \quad C(u) = C(v)
	\end{align*}
	Non singular code is defined as opposite
\end{definition}

\begin{definition}
	A code $\C$ is called \textit{uniquily decodable} if
	\begin{align*}
		\forall u_1,...,u_n,v_1,...,v_n \in \U^* \quad s.t. \quad u_1,...,u_n \neq v_1,...,v_n
	\end{align*}
	we have
	\begin{align*}
		\C(u_1) \C(u_n) \neq \C(v_1) \C(v_n)
	\end{align*}
	i.e, $\C^*$ is non-singular
\end{definition}

\begin{definition}
	Suppose $\C : \U \rightarrow \{ 0, 1\}^*$ and $\D : \V \rightarrow \{ 0, 1\}^*$ we can define

	\begin{align*}
		\C \times \D : \U \times \V \rightarrow \{0, 1\}^*
	\end{align*}
	as
	\begin{align*}
		(\C \times \D)(u, v) \rightarrow \C(u)\D(v)
	\end{align*}
\end{definition}

\begin{definition}
	Given $\C : \U \rightarrow \{ 0, 1\}^*$, define
	\begin{align*}
		\C^* : \U^* \rightarrow \{0, 1\}^*
	\end{align*}
	as
	\begin{align*}
		\C^*(u_1, u_n) = \C(u_1)...\C(u_n)
	\end{align*}
\end{definition}
% ----------------------------------------
% jb markdown


\subsubsection{Markov chains}\label{markov-chains}

For a Markov Chaine $A \to B \to C \to D$, the joint probability
distribution of the RVs should be $p(a)p(b|a)p(c|b)p(d|c)$

\begin{itemize}
\item
  The reverse of a MC is a MC
\end{itemize}

\paragraph{Kraft-sum}\label{kraft-sum}

\textbf{Definition:} The Kraftsum of a code $C$ is
$KS(C) = \sum_u 2^{-|C(u)|}$

\begin{itemize}
\item
  if $C$ is prefix free then $KS(C) \leq 1$
\item
  if $C​$ is non singular, then $KS(C) \leq 1 + \min_u |C(u)|​$
\item
  $KS(C^n) = KS(C)^n$
\end{itemize}

\textbf{Theorem:} for any $U$ and associated $p(u)$ there exists a
prefix free code $C$ s.t.

\[E[|C(U)|] < 1 + \sum_{u\in U} p(u) \log \frac 1 {p(u)}\]

\textbf{Theorem:} if $KS(C)\leq 1$ then there exists a prefix free
code $C'$ such that $|C(u)| = |C'(u)|$ for all $u$

\textbf{Corollar:} if $C$ is uniquely decodable, then there exists
$C'$ that is prefix free with the same word lengths

\paragraph{Entropy}\label{entropy}

\textbf{Definition:} the entropy of a random variable $U$ is

\[H(U) = \sum_{u\in U} p(u) \log \frac 1 {p(u)} = E_U\left[\log \frac 1 {p(u)}\right]\]

\textbf{Theorem:} if $C$ is uniquely decodable then
$E[|C(U)|] \geq H(U)$

\paragraph{Properties of optimal prefix free
codes}\label{properties-of-optimal-prefix-free-codes}

\begin{enumerate}
\item
  $p(u) < p(v) \to |u| \geq |v|$
\item
  The two longest codewords have the same length
\item
  The 2 least probable letters are assigned codewords that differ in the
  last bit
\end{enumerate}

\subsubsection{Hoffman algorithm}\label{hoffman-algorithm}

\begin{itemize}
\item
  Combine the 2 least likely symbols
\item
  Sum their probability and assign it a new fictive symbol
\item
  Repeat
\end{itemize}

\begin{definition}[Joint entropy]
  Suppose $U, V$ are Random Variables with $p(u,v) = P(U=u, V=v)$, the joint entropy is
  \[
    H(UV) = \sum_{u,v} p(u,v) \log \frac 1 {p(u,v)}
  \]
\end{definition}

\begin{theorem}
  \[
    H(UV) \leq H(U) + H(V)
  \]
  with equality iff $U$ and $V$ are independants.
\end{theorem}

\begin{proof}
  We want to show that
  \begin{align*}
    \sum_{u,v} p(u,v) \log \frac 1 {p(u,v)} \leq \sum_u p(u) \log \frac 1 {p(u)} + \sum_v p(v) \log \frac 1 {p(v)}
    \iff \sum_{u,v} p(u,v) \log \frac {p(u)p(v)} {p(u,v)} \leq 0
  \end{align*}
  We use $\ln z \leq z - 1~\forall z$ (with equality iff $z=1$):
  \begin{align*}
    \sum_{u,v} p(u,v) \log \frac {p(u)p(v)} {p(u,v)} \leq \sum_{u,v} p(u,v) \left[ \frac {p(u)p(v)} {p(u, v)} - 1 \right] = \sum_{u,v} p(u)p(v) - \sum_{u,v} p(u,v) = 1 - 1 = 0
  \end{align*}
\end{proof}

Same definitions of entropy holds for $n$ symbols.

\begin{definition}[Joint Entropy]
  Suppose $U_1, U_2, \dots, U_n$ are RVs and we are given $p(u_1 \dots u_n)$, the joint entropy is
  \[
    H(U_1, \dots, U_n) = \sum_{u_1 \dots u_n} p(u_1 \dots u_n) \log \frac 1 {p(u_1 \dots u_n)}
  \]
\end{definition}

\begin{theorem}
  \[
    H(U_1, \dots, U_n) \leq \sum_{i=1}^n H(U_i)
  \]
  with equality iff $U$s are independants
\end{theorem}

\begin{corollary}
  if $U_1, \dots, U_n$ are i.i.d. then $H(U_1 \dots U_n) = nH(U_1)$
\end{corollary}

\begin{definition}[Conditional entropy]
  \[
    H(U|V) = \sum_{u,v} p(u,v) \log \frac 1 {p(u|v)}
  \]
\end{definition}

\begin{theorem}
  \[
    H(UV) = H(U) + H(V|U) = H(V) + H(U|V)
  \]
\end{theorem}

\begin{theorem}
  \[
    H(U) + H(V) \geq H(U, V) = H(V) + H(U|V)
  \]
\end{theorem}

\begin{definition}[Mutual information]
  \begin{align*}
  I(U;V) = I(V;U) &= H(U) - H(U|V)\\
  &= H(V) - H(V|U)\\
  &= H(U) + H(V) - H(UV)
  \end{align*}
\end{definition}

We can apply the chain rule on the entropy as follow

\[
  H(U_1, U_2, \dots U_n) = H(U_1) + H(U_2|U_1) + \dots + H(U_n|U_1,U_2 \dots U_{n-1})
\]


\begin{definition}[Conditional mutual information]
  \begin{align*}
    I(U;V|W) &= H(U|W) - H(U|VW)\\
    &= H(V|W) - H(V|UW)
  \end{align*}
\end{definition}

\begin{theorem}
  \[
    I(V;U_1\dots U_n) = I(V;U_1) + I(V;U_2 | U_1) + \dots + I(V;U_n|U_1 \dots U_{n-1})
  \]
\end{theorem}

\end{document}
