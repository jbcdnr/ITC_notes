\documentclass{article}
% General document formatting
\usepackage[margin=1in]{geometry}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{amsthm}
\usepackage{cleveref}
\usepackage{xcolor}
\usepackage[toc,page]{appendix}

\title{Information Theory and Coding - Prof.~Emere Telatar}
\date{\today}
\author{Jean-Baptiste Cordonnier, Sebastien Speierer, Thomas Batschelet}

% define example environments
\newcounter{example}[section]
\newenvironment{example}
    {
    \refstepcounter{example}
    \begin{center}
	    \begin{tabular}{|p{0.8\textwidth}|}
		    \hline\\
			\textbf{Example~\theexample.}\\
		    }
		    {
		    \\\hline
	    \end{tabular}
    \end{center}
    }

% define definition environments
% \newcounter{definition}[section]
% \newenvironment{definition}
%     {
%     \refstepcounter{definition}
%     \begin{center}
% 	    \begin{tabular}{|p{0.8\textwidth}|}
% 		    \hline\\
% 			\textbf{Definition~\thedefinition.}\\
% 		    }
% 		    {
% 		    \\\hline
% 	    \end{tabular}
%     \end{center}
%     }

\newtheorem{theorem}{Theorem}[section]
\newtheorem{corollary}{Corollary}[theorem]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{definition}{Definition}[section]
\newtheorem{proposition}{Proposition}[section]
\newtheorem{notation}{Notation}

\newcommand{\todo}[1]{\textcolor{red}{TODO: #1}}

% math bold
\def\*#1{\mathbf{#1}}
% manuscript characters
\def\D{\mathcal{D}}
\def\V{\mathcal{V}}
\def\L{\mathcal{L}}
\def\U{\mathcal{U}}
\def\N{\mathcal{N}}
\def\X{\mathcal{X}}
\def\C{\mathcal{C}}

\DeclareMathOperator{\E}{\mathbb{E}}% expected value


\begin{document}

\maketitle


\section{Data compression}

Given an alphabet $\U$ (e.g. $\U = \{a, ..., z, A, ..., Z, ...\}$), we want to assign binary sequences to elements of $\U$, i.e.

\begin{align*}
	e: \U \rightarrow {0, 1}^* = \{\emptyset, 0, 1, 00, 01, ...\}
\end{align*}

For $\X$ a set

\begin{align*}
	\X^n &\equiv \{ (x_0 ... x_n), x_i \in \X\} \\
	\X^* &\equiv \bigcup_{n \geq 0} \X^n
\end{align*}

\begin{definition}
	A code $\C$ is called \textit{singular} if
	\begin{align*}
		\exists (u, v) \in \U^2, u \neq v \quad s.t. \quad C(u) = C(v)
	\end{align*}
	Non singular code is defined as opposite
\end{definition}

\begin{definition}
	A code $\C$ is called \textit{uniquily decodable} if
	\begin{align*}
		\forall u_1,...,u_n,v_1,...,v_n \in \U^* \quad s.t. \quad u_1,...,u_n \neq v_1,...,v_n
	\end{align*}
	we have
	\begin{align*}
		\C(u_1) \C(u_n) \neq \C(v_1) \C(v_n)
	\end{align*}
	i.e, $\C^*$ is non-singular
\end{definition}

\begin{definition}
	Suppose $\C : \U \rightarrow \{ 0, 1\}^*$ and $\D : \V \rightarrow \{ 0, 1\}^*$ we can define

	\begin{align*}
		\C \times \D : \U \times \V \rightarrow \{0, 1\}^*
		\quad \text{ as } \quad
		(\C \times \D)(u, v) \rightarrow \C(u)\D(v)
	\end{align*}
\end{definition}

\begin{definition}
	Given $\C : \U \rightarrow \{ 0, 1\}^*$, define
	\begin{align*}
		\C^* : \U^* \rightarrow \{0, 1\}^* 
		\quad \text{ as } \quad
		\C^*(u_1, u_n) = \C(u_1)...\C(u_n)
	\end{align*}
\end{definition}

\begin{definition}
	A code $\U \rightarrow \{0, 1\}^*$ is \textbf{prefix free} is for no $u \neq v$ $\C(u)$ is a prefix of $\C(v)$.
\end{definition}

\begin{theorem}
	If $\C$ is prefix free then $\C$ is uniquely decodable.
\end{theorem}


\begin{definition}
	\textbf{Kraft sum}: Given $\C : \U \rightarrow \{ 0, 1\}^*$
	\begin{align*}
		kraftsum(\C) = \sum 2^{length(\C(u))}
	\end{align*}
\end{definition}

\begin{lemma}
	if $\C : \U \rightarrow \{ 0, 1\}^*$ and $\D : \V \rightarrow \{ 0, 1\}^*$
	then $kraftsum(\C \times \D) = kraftsum(\C)j \times kraftsum(\D)$
	\begin{proof}
    \begin{align*}
      kraftsum(\C \times \D) &= \sum_{u, v} 2^{-(length(\C) * length(\D))} \\
      &= \sum_u 2^{-length(\C)} \sum_v 2^{- length(\D)}
    \end{align*}
	\end{proof}
\end{lemma}

\begin{corollary}
  $kraftsum(\C^n) = (kraftsum(\C))^n$
\end{corollary}

\begin{proposition}
  if $\C$ is non-singular, then 
  \begin{align*}
    kraftsum(\C) \leq 1 + \max_n length(\C(u))
  \end{align*}
\end{proposition}

\begin{theorem}
  if $\C$ is uniquely decodable, then $kraftsum(\C) \leq 1$
\end{theorem}

\begin{proof}
  $\C$ is uniquely decodable $\equiv$ $\C^*$ is non singular
  \begin{align*}
    &\Rightarrow kraftsum(\C^n) \leq 1 + \max _{u_1, ..., u_n} length(\C^n) \\
    &\Rightarrow kraftsum(\C)^n \leq 1 + n L, \quad L = \max length(\C(n))
  \end{align*}
  A growing exp cannot be bounded by a linear function 
  \begin{align*}
    \Rightarrow kraftsum(\C) \leq 1
  \end{align*}
\end{proof}

\begin{theorem}
  Suppose $\C : \U \rightarrow \N$ is such that $\sum_u i^{\C(u)} \leq 1$, then, there exist a prefix-free code $\C: \U \rightarrow \{0, 1\}$ s.t. $\forall length(\C(u)) = \C(u)$
\end{theorem}

\begin{proof}
  Let $\U = \{u_1, ..., u_n\}$ and $\C(u_1) \leq \C(u_2) \leq ... \leq \C(u_k) = \C_{max}$.
  Consider the complete binary tree up to depth $\C_{max}$ initially all nodes are available to be used as codewords.
  For $i = 1, 2, ..., n$, place $\C(u_i)$ at an available node at level $\C(u_i)$ remove all descendant of $\C(u_i)$ from the available list.

  \begin{corollary}
    Suppose $\C: \U \rightarrow \{0, 1\}^*$ is u.d., then there exist an $\C': \U \rightarrow \{0, 1\}^*$ which is prefix-free and $length(\C'(n)) = length(\C(n))$
  \end{corollary}
\end{proof}

\begin{example}
  $\U = \{a, b, c, d\}$, $\C: \{0, 01, 011, 111\}$ and $\C': \{0, 10, 110, 111\}$\\
  In this case, decoding $\C$ may require delay, while decoding $\C'$ is instanteneous.
\end{example}

% -------------------------------------------------------------

\section{Alphabet with statistics}

Suppose we have an alphabet $\U$, and suppose we have a random variable $\U$ taking values in $\U$. We denote by $p(u) = Pr(U = u), u \in \U$ with $p(u) \geq 0$ and $\sum_u p(u) = 1$.\\

Suppose we have a code $\C: \U \rightarrow \{0, 1\}^*$. We then have $\C(u)$ a random binary string and $length(\C(u))$ a random integer.

\begin{example}
  $\U = \{a, b, c, d\}$\\
  $p: \{0.5, 0.25, 0.125, 0.125\}$ \\
  $\C: \{0, 01, 110, 111\}$ \\

  then we have
  \begin{align*}
    length(\C(u)) = 
    \left\{
      \begin{array}{l}      
        1, \quad p = 0.5 \\
        2, \quad p = 0.25 \\
        3, \quad p = 0.125 + 0.125 + 0.25
      \end{array}
    \right.
  \end{align*}
\end{example}

We can measure how efficient $\C$ represents $\U$ by considering
\begin{align*}
  E[length(\C(u))] = \sum_u p(u) \C(u) \quad \text{with} \quad \C(u) = length(\C(u))
\end{align*}

\begin{theorem}
  if $\C$ is u.d., then 
  \begin{align*}
    E[length(\C(u))] \geq \sum_u p(u) \log(\frac{1}{p(u)})
  \end{align*}
\end{theorem}

\begin{proof}
  let $\C(u) = length(\C(u))$, we know $\sum_u 2^{-\C(u)} \leq 1$ because $\C$ is u.d.

  \begin{align*}
    E[length(\C(u))] &= \sum_u p(u) \C(u) = \sum_u p(u) \log_2(\frac{1}{q(u)}) \\
    &\equiv \sum_u p(u) \log(\frac{q(u)}{p(u)}) \leq 0 \\
    &\equiv \sum_u p(u) \ln(\frac{q(u)}{p(u)}) \leq 0 \\
    &\leq \sum_u p(u) [\frac{q(u)}{p(u)} - 1]
    = \underbrace{\sum_u q(u)}_{\leq 1} - \underbrace{\sum_u p(u)}_{=1} \leq 0
  \end{align*}
\end{proof}

\begin{theorem}
  For any $\U$, there exists a prefix-free code $\C$ s.t.
  \begin{align*}
    E[length(\C(u))] < 1 + \sum_{u \in \U} p(u) \log(\frac{1}{p(u)})
  \end{align*}
\end{theorem}
\begin{proof}
  Given $\U$, let 
  \begin{align*}
    &\C(u) = [\log(\frac{1}{p(u)})] < 1 + \log(\frac{1}{p(u)}) \\
    &\Rightarrow \sum_u 2^{-\C(u)} \leq \sum_u p(u) = 1 \\
    &\Rightarrow \sum_u p(u) \C(u) < \sum_u p(u) \log(\frac{1}{p(u)}) + \underbrace{1}_{\sum p(u)}
  \end{align*}
\end{proof}

\begin{theorem}
  The entropy of a RV $U \in \U$ is 
  \begin{align*}
    H(U) = \sum_{u \in \U} p(u) \log(\frac{1}{p(u)})
  \end{align*}
  with $p(u) = Pr(U = u)$
\end{theorem}
Note that $H(U)$ is a fonction of the distribution $\C_u(.)$ of the RV $U$, it isn't a function of $U$.

\begin{align*}
  H(U) = E[f(U)] \quad \text{where} \quad \log(\frac{1}{p(u)})
\end{align*}

How to design optimal codes (in the sense of minimizing $E[length(\C(u))]$)? \\
Formally, given a random variable $U$, find $\C(u) \rightarrow \N$ s.t.
\begin{align*}
  \sum_{u \in U} 2^{\C(u)} \leq 1 
\quad \text{that minimizes} \quad
  \sum_{u \in U} p(u)\C(u)
\end{align*}

Properties of optimal prefix-free codes
\begin{itemize}
  \item if $p(u) < p(v)$ then $\C(u) \geq \C(v)$
  \item The two longest codewords have the same length
  \item There is an optimal code such that the two least probable letters are assigned codewords that differ in the last bit.
\end{itemize}

Observe that if $\C(u_1), ... , \C(u_{k-1}), \C(u_k)$ is a prefix-free collection of the property that
\begin{align*}
\left.
\begin{array}{l l}
  \C(u_{k-1}) &= \alpha 0 \\
  \C(u_k)     &= \alpha 1 
\end{array}
\right.
\quad \text{with} \quad \alpha \in \{ 0, 1\}^*
\end{align*}

then $\{\C(u_1), ..., \C(u_{k-2}, \alpha\}$ is also a prefix-free collection.

Also
\begin{align*}
  \sum_{u \in \U} p(u) length(\C(u)) &= p(u_1) length(\C(u_1)) + ... +  p(u_{k-2}) length(\C(u_{k-2}))
  + [p(u_{k-1}) + p(u_k)](length(\alpha) + 1) \\
  &= (p(u_{k-1}) + p(u_k)) + \sum_{v \in \V} p(v) length(\C'(v))
\end{align*}

So we have shown that with
\begin{align*}
  E[length(\C(U)] = p(u_{k-1}) + p(u_k) + E[length(\C'(v))]
\end{align*}
if $\C$ is optimal for $U$, then $\C'$ is optimal for $V$

% -------------------------------------------------------------

\section{Entropy and mutual information}
\label{sec:entropy}

\begin{definition}[Joint entropy]
  Suppose $U, V$ are Random Variables with $p(u,v) = P(U=u, V=v)$, the joint entropy is
  \[
    H(UV) = \sum_{u,v} p(u,v) \log \frac 1 {p(u,v)}
  \]
\end{definition}

\begin{theorem}
  \[
    H(UV) \leq H(U) + H(V)
  \]
  with equality iff $U$ and $V$ are independants.
\end{theorem}

\begin{proof}
  We want to show that
  \begin{align*}
    \sum_{u,v} p(u,v) \log \frac 1 {p(u,v)} \leq \sum_u p(u) \log \frac 1 {p(u)} + \sum_v p(v) \log \frac 1 {p(v)}
    \iff \sum_{u,v} p(u,v) \log \frac {p(u)p(v)} {p(u,v)} \leq 0
  \end{align*}
  We use $\ln z \leq z - 1~\forall z$ (with equality iff $z=1$):
  \begin{align*}
    \sum_{u,v} p(u,v) \log \frac {p(u)p(v)} {p(u,v)} \leq \sum_{u,v} p(u,v) \left[ \frac {p(u)p(v)} {p(u, v)} - 1 \right] = \sum_{u,v} p(u)p(v) - \sum_{u,v} p(u,v) = 1 - 1 = 0
  \end{align*}
\end{proof}

Same definitions of entropy holds for $n$ symbols.

\begin{definition}[Joint Entropy]
  Suppose $U_1, U_2, \dots, U_n$ are RVs and we are given $p(u_1 \dots u_n)$, the joint entropy is
  \[
    H(U_1, \dots, U_n) = \sum_{u_1 \dots u_n} p(u_1 \dots u_n) \log \frac 1 {p(u_1 \dots u_n)}
  \]
\end{definition}

\begin{theorem}
  \[
    H(U_1, \dots, U_n) \leq \sum_{i=1}^n H(U_i)
  \]
  with equality iff $U$s are independants
\end{theorem}

\begin{corollary}
  if $U_1, \dots, U_n$ are i.i.d. then $H(U_1 \dots U_n) = nH(U_1)$
\end{corollary}

\begin{definition}[Conditional entropy]
  \[
    H(U|V) = \sum_{u,v} p(u,v) \log \frac 1 {p(u|v)}
  \]
\end{definition}

\begin{theorem}
  \[
    H(UV) = H(U) + H(V|U) = H(V) + H(U|V)
  \]
\end{theorem}

\begin{theorem}
  \[
    H(U) + H(V) \geq H(U, V) = H(V) + H(U|V)
  \]
\end{theorem}

\begin{definition}[Mutual information]
  \begin{align*}
  I(U;V) = I(V;U) &= H(U) - H(U|V)\\
  &= H(V) - H(V|U)\\
  &= H(U) + H(V) - H(UV)
  \end{align*}
\end{definition}

We can apply the chain rule on the entropy as follow

\[
  H(U_1, U_2, \dots U_n) = H(U_1) + H(U_2|U_1) + \dots + H(U_n|U_1,U_2 \dots U_{n-1})
\]


\begin{definition}[Conditional mutual information]
  \begin{align*}
    I(U;V|W) &= H(U|W) - H(U|VW)\\
    &= H(V|W) - H(V|UW)\\
    &= \E_{u,v,w} \left[ \log \frac {p(uv|w)} {p(u|w)p(v|w)} \right]
  \end{align*}
\end{definition}

\begin{theorem}
  \[
    I(V;U_1\dots U_n) = I(V;U_1) + I(V;U_2 | U_1) + \dots + I(V;U_n|U_1 \dots U_{n-1})
  \]
\end{theorem}

\begin{notation}
  \[
    U^n \triangleq (U_1, U_2, \dots U_n)
  \]
\end{notation}

\begin{theorem}
  \[
    I(U;V |W) \geq 0
  \]
  equality iff conditioned on $w$, $u$ and $v$ are independant, that is iff $U-V-W$ is a Markov chain.
\end{theorem}

\begin{proof}
  \begin{align*}
    I(U;V|W) &= \frac 1 {\ln 2} \sum_{u,v,w} p(u,v,w) \ln \frac {p(u|w)p(v|w)} {p(uv | w)}\\
    &\geq \frac 1 {\ln 2} \sum_{u,v,w} p(u,v,w) \left[ \frac {p(u|w)p(v|w)} {p(uv | w)} - 1 \right]\\
    &=\frac 1 {\ln 2} \sum_{u,v,w}(p(w)p(u|w)p(v|w) - p(uvw))\\
    &= \frac 1 {\ln 2}(1 - 1) \\
    &=0
  \end{align*}
\end{proof}


\section{Data processing}

\begin{theorem}
  $U-V-W$ is a MC $\iff I(U;W|V) = 0$
\end{theorem}

\begin{corollary}
  $I(U;V) \geq I(U;W)$ and by symetry of MC $I(W;V) \geq I(U;W)$
\end{corollary}

\begin{proof}
  \[
    I(U;VW) = I(U;V) + I(U;W|V) = I(U;V)
  \]
  and
  \[
    I(U;VW) = I(U;W) + I(U;V|W) \geq I(U;W)
  \]
\end{proof}

\begin{theorem}
  Given $U$ a RV taking values in $\U$ then $0 \leq H(U) \leq \log | \U |$. $H(U)=0$ iff $U$ is constant, $H(U)=\log | \U |$ iff $U$ is $p(u) = 1 / |\U|$ for all $u$.
\end{theorem}

\begin{proof}
For the lower bound,
  \[
    H(U) = \sum_u \underbrace{p(u)}_{\geq 0} \underbrace{\log \frac 1 {p(u)}}_{\geq 0} \geq 0
  \]
For the upper bound,

\begin{align*}
  H(U) - \log | \U |
  &= \sum_u p(u) \log \frac 1 {p(u)} - \sum_u p(u) \log |\U|\\
  &= \frac 1 {\ln 2} \sum_u p(u) \ln \frac 1 {|\U | p(u)}\\
  &\leq \frac 1 {\ln 2} \sum_u p(u) \left(\frac 1 {|\U | p(u)} - 1 \right)\\
  &=\frac 1 {\ln 2} \left[ \sum_u \frac 1 {|\U |} - \sum_u p(u) \right]\\
  &=0
\end{align*}
\end{proof}

\begin{theorem}
  $I(U;V) = 0 \iff U \bot V$
\end{theorem}


\begin{definition}[Entropy rate of a stochastic process]
  $\lim_{n\to \infty} \frac 1 n H(U^n)$ if the limit exists.
\end{definition}

\begin{theorem}
  For stationary stochastic process $U^n$, the sequences
  \[
    a_n = \frac 1 n H(U^n) \text{ and } b_n = H(U_n|U^{n-1})
  \]
  are positive and non increasing. Then $a=\lim_{n\to \infty} a_n$ and $b=\lim_{n\to \infty} b_n$ exists and $a=b$.
\end{theorem}

\begin{proof}
\begin{align*}
  b_{n+1}
  &= H(U_{n+1}|U_1, U_2,\dots,U_n)\\
  &\leq H(U_{n+1}|U_2,\dots,U_n)\\
  &= H(U_n|U_1, U_2,\dots,U_{n-1})\\
  &= b_n \text{ , because $U_1 \dots U_n \sim U_2 \dots U_{n+1}$ (Stationarity).}
\end{align*}


Hence, it is non-increasing.\\\\

For the \{$a_n$\}, observe that

\begin{align*}
a_n = \frac{1}{n} H(U^n) 
&= \frac{1}{n} \bigg[ H(U_1)+H(U_2|U_1)+H(U_3|U^2) +\dots+H(U_n|U^{n-1}) \bigg]\\
&= \frac{1}{n} \bigg[ b_1+b_2+\dots+b_n\bigg]
\end{align*}


and by the "Lemma", whenever $b_n \rightarrow b$ , \space $a_n \rightarrow b$
\end{proof}
\begin{lemma}[Cesaro]
	Suppose $b_n \rightarrow b$, \\\\
	
	then, 
	\[
	a_n = \frac{1}{n} \bigg[ b_1+b_2+\dots+b_n\bigg] \text{ also converges and to 1.}
	\]
\end{lemma}
\begin{proof}
	Since $b_n \rightarrow b$ , $\bigg( \equiv \forall \epsilon > 0$ , $\exists$ $n(\epsilon)$ s.t $ \forall n > n(\epsilon)$  $ |b_n-b| < \epsilon\bigg)$\\
	
	$\exists B $ s.t. $ |b_n| < B$ for all n.\\\\
	Take $n > n_1(\epsilon) \triangleq \dots $ then
	\[
		|a_n-b| \leq \frac{|b_1-b|+|b_2-b|+|b_3-b|+\dots+|b_n-b|}{n}
	\]
	\[
		\text{so  }  |a_n-b| \leq \frac{1}{n}\bigg[ \sum_{i=1}^{n_0(\epsilon)} \underbrace{|b_i-b|}_{2B}  + \sum_{i=n_0(\epsilon)+1}^{n} \underbrace{|b_i-b|}_{\leq \epsilon}\bigg] \leq \frac{n_0(\epsilon) 2B}{n} + \epsilon < 2\epsilon
	\]
	\[
		\text{for } n>n_1(\epsilon) \triangleq \text{max, } \{ n_0(\epsilon) \frac{1}{\epsilon} n_0(\epsilon) 2B \}
	\]
\end{proof}



% -------------------------------------------------------------------------
% class 9.10.2017

\begin{theorem}
  Given a stationary process with entropy rate $r$:
  \begin{align*}
    r = \lim_{n \rightarrow \infty} \frac{1}{n} H(\U^n)
  \end{align*}

  then 
  \begin{enumerate}
    \item for every source coding scheme
    \begin{align*}
      \C_n: \U^n \rightarrow \{0, 1\}^*
    \end{align*}
    the expected number of bits / letter is given by
    \begin{align*}
      \frac{1}{n} E[length(\C(\U^n))] \geq r
    \end{align*}
    \item for any $\epsilon > 0$, there exists a source coding scheme $\C_n: \U^n \rightarrow \{0, 1\}^*$ s.t.
    \begin{align*}
      \frac{1}{n} E[length(\C_n(\U^n))] < r + \epsilon
    \end{align*}
  \end{enumerate}
\end{theorem}

\begin{proof}  
  \begin{enumerate}
    \item we already know
    \begin{align*}
      \frac{1}{n} E[length(\C_n(\U^n))] \geq \frac{1}{n} H(\U_1 ... \U_n)
    \end{align*}
    and the right term is decreasing
    \item we also know that for each $n, \exists \C_n$ that is prefix-free s.t.
    \begin{align*}
      E[length(\C_n(U^n))] < \underbrace{\frac{1}{n} H(\U^n)]}_{r} + \underbrace{\frac{1}{n}}_{0}
    \end{align*}
    we can find $n$ large enough s.t. the RHS $< r + \epsilon$
  \end{enumerate}
\end{proof}

\section{Typicality and typical set}

Suppose we have a sequence $U_1, U_2, ...$ of i.i.d. random variables taking values in a n alphabet $\U$.
Suppose we observe $u_1,u_2..., u_n$. We will call it to be \textit{typical-$(\epsilon, p)$} if 
\begin{align*}
  p(u) (1 - \epsilon) 
  \leq \frac{\# \text{ of times $u$ apperas in $u_1, ..., u_n$}}{n}
  \leq p(u)(1+\epsilon)
\end{align*}

\begin{theorem}
  $u^n$ is $(\epsilon, p)$-typical then
  \begin{align*}
    2^{-n H(u)(1 + \epsilon)}
    \leq Pr(U^n = u^n)
    \leq 2^{-n H(u)(1 + \epsilon)}
  \end{align*}
\end{theorem}

\begin{proof}
  \begin{align*}
    Pr(U^n = u^n) &= \prod_{i=1}^n Pr(U_i = u_i) = \prod_{i=1}^n p(u_i) = \prod_{u \in U} p(u)^{\#_u}
  \end{align*}
  with $\#_u$ the number of times $u$ appears in $u_1, ..., u_n$ where
  \begin{align*}
    n (1-\epsilon) p(u) \leq \#_u \leq n(1+\epsilon)p(u)
  \end{align*}
  consequently
  \begin{align*}
    p(u)^(n p(u)(1-\epsilon)) \geq p(u)^{\#_u} \geq p(u)^{n p(u)(1+\epsilon)}
  \end{align*}
  then
  \begin{align*}
    (\prod_{n} p(u)^{p(u)})^{(1-\epsilon)n}
    \geq Pr(U^n = u^n)
    \geq (\prod_{n} p(u)^{p(u)})^{(1+\epsilon)n}
  \end{align*}
  but 
  \begin{align*}
    p(u)^{p(u)} = 2^{-p(u) \log(\frac{1}{p(u)})} \Rightarrow \prod p(u)^{p(u)} = 2^{-H(u)}
  \end{align*}
\end{proof}

\begin{definition}{Typical set}
  \begin{align*}
    T(n, \epsilon, p) = \{ u^n \in U^n : u^n \text{ is } (\epsilon, p)\text{-typical}\}
  \end{align*}
\end{definition}

\begin{theorem}
  \begin{enumerate}
    \item if $u^n \in T(n, \epsilon, p)$ then
    \begin{align*}
      p(u^n) = Pr(U^n = u^n) = 2^{-n H(u)(1 \pm \epsilon)}
    \end{align*}  
    when $U_i$ i.i.d.
    \item 
    \begin{align*}
      \lim_{n \rightarrow \infty} Pr(U^n \in T(n, \epsilon, p)) = 1
    \end{align*}
    \item 
    \begin{align*}
      |T(n, \epsilon, p)| \leq 2^{n (H(u)(1 + \epsilon))}
    \end{align*}
    \item 
    \begin{align*}
      |T(n, \epsilon, p)| \geq (1-\epsilon) 2^{n H(u)(1-\epsilon)}
    \end{align*}
  \end{enumerate}
\end{theorem}

\begin{proof}
  TODO
\end{proof}


% -------------------------------------------------------------------------

\newpage
\begin{appendices}
\section{Markov chains}
\label{appendix:markov-chains}

$U_1 - U_2 - \dots - U_n$ forms a Markov chain if the joint probability
distribution of the RVs is
\[
  p(a,b,c,d) = p(a)p(b|a)p(c|b)p(d|c)
\]
which is equivalent to $(U_1, \dots, U_{k-1})$ are independant of $(U_{k+1}, \dots, U_n)$ when conditionned on $U_k$ for any $k$.


\begin{theorem}
  The reverse of a MC is a MC
\end{theorem}


\section{Stochastic processes}
\label{appendix:stoch-proc}

A stochastic process is a collection $U_1, U_2 \dots U_n$ of RVs each taking values in $\U$. It is described by its joint probability
\[
  p(u^n) = P(U_1 \dots U_n = u_1 \dots u_n) = P(U^n = u^n)
\]

\begin{definition}[Stationary stochastic process]
  A process $U_1, U_2, \dots$ is called stationary if for every $n$ and $k$ and $u_1 \dots u_n$, we have
  \[
    p(u^n) = p(U_1 \dots U_n = u_1 \dots u_n) = p(U_{1+k} \dots U_{n+k} = u_1 \dots u_n)
  \]
  In other words, the process is time shift invariant.
\end{definition}

\end{appendices}


\end{document}
