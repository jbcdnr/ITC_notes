\documentclass{article}
% General document formatting
\usepackage[margin=1in]{geometry}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{amsthm}
\usepackage{mathrsfs}
\usepackage{cleveref}
\usepackage{xcolor}
\usepackage[toc,page]{appendix}

\title{Information Theory and Coding - Prof.~Emere Telatar}
\date{\today}
\author{Jean-Baptiste Cordonnier, Sebastien Speierer, Thomas Batschelet}

% define example environments
% \newcounter{example}[section]
% \newenvironment{example}
%     {
%     \refstepcounter{example}
%     \begin{center}
% 	    \begin{tabular}{|p{0.8\textwidth}|}
% 		    \hline\\
% 			\textbf{Example~\theexample.}\\
% 		    }
% 		    {
% 		    \\\hline
% 	    \end{tabular}
%     \end{center}
%     }

% define definition environments
% \newcounter{definition}[section]
% \newenvironment{definition}
%     {
%     \refstepcounter{definition}
%     \begin{center}
% 	    \begin{tabular}{|p{0.8\textwidth}|}
% 		    \hline\\
% 			\textbf{Definition~\thedefinition.}\\
% 		    }
% 		    {
% 		    \\\hline
% 	    \end{tabular}
%     \end{center}
%     }

\newtheorem{theorem}{Theorem}[section]
\newtheorem{corollary}{Corollary}[theorem]
\newtheorem{claim}{Claim}[section]
\newtheorem{observation}{Observation}[section]
\newtheorem*{wikipedia}{Wikipedia}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{definition}{Definition}[section]
\newtheorem{proposition}{Proposition}[section]
\newtheorem{notation}{Notation}
\theoremstyle{definition} % Attention side effects lololol~
\newtheorem{example}{Example}

\newcommand{\todo}[1]{\textcolor{red}{TODO: #1}}
\renewcommand{\Pr}[1]{Pr\left\{#1\right\}}
\newcommand{\Ex}[1]{E\left[#1\right]}
\newcommand{\pfrac}[2]{\left( \frac{#1}{#2} \right)}

% math bold
\def\*#1{\mathbf{#1}}
% manuscript characters
\def\D{\mathcal{D}}
\def\V{\mathcal{V}}
\def\L{\mathcal{L}}
\def\U{\mathcal{U}}
\def\N{\mathcal{N}}
\def\X{\mathcal{X}}
\def\Y{\mathcal{Y}}
\def\R{\mathbb{R}}


\def\C{\mathscr{C}}
\DeclareMathOperator{\E}{\mathbb{E}}% expected value


\begin{document}

\maketitle

\section{Data compression}

\begin{definition}[Information]
  Abstractly, \textbf{information} can be thought of as the resolution of uncertainty.
\end{definition}

Given an alphabet $\U$ (e.g. $\U = \{a, ..., z, A, ..., Z, ...\}$), we want to assign binary sequences to elements of $\U$, i.e.

\begin{align*}
	\C: \U \rightarrow \{0, 1\}^* = \{\emptyset, 0, 1, 00, 01, ...\}
\end{align*}

For $\X$ a set

\begin{align*}
	\X^n &\equiv \{ (x_0 ... x_n), x_i \in \X\} \\
	\X^* &\equiv \bigcup_{n \geq 0} \X^n
\end{align*}

\begin{definition}
	A code $\C$ is called \textbf{singular} if
	\begin{align*}
		\exists (u, v) \in \U^2, u \neq v \quad s.t. \quad C(u) = C(v)
	\end{align*}
	Non singular code is defined as opposite
\end{definition}

\begin{definition}
	A code $\C$ is called \textbf{uniquely decodable} if
	\begin{align*}
		\forall u_1,...,u_n,v_1,...,v_n \in \U^* \quad s.t. \quad u_1,...,u_n \neq v_1,...,v_n
	\end{align*}
	we have
	\begin{align*}
		\C(u_1)...\C(u_n) \neq \C(v_1)...\C(v_n)
	\end{align*}
	i.e, $\C$ is non-singular
\end{definition}

\begin{definition}
	Suppose $\C : \U \rightarrow \{ 0, 1\}^*$ and $\D : \V \rightarrow \{ 0, 1\}^*$ we can define

	\begin{align*}
		\C \times \D : \U \times \V \rightarrow \{0, 1\}^*
		\quad \text{ as } \quad
		(\C \times \D)(u, v) \rightarrow \C(u)\D(v)
	\end{align*}
\end{definition}

\begin{definition}
	Given $\C : \U \rightarrow \{ 0, 1\}^*$, define
	\begin{align*}
		\C^* : \U^* \rightarrow \{0, 1\}^*
		\quad \text{ as } \quad
		\C^*(u_1 ... u_n) = \C(u_1)...\C(u_n)
	\end{align*}
\end{definition}

\begin{definition}
	A code $\U \rightarrow \{0, 1\}^*$ is \textbf{prefix-free} is for no $u \neq v$ $\C(u)$ is a prefix of $\C(v)$.
\end{definition}

\begin{theorem}
	If $\C$ is prefix-free then $\C$ is uniquely decodable.
\end{theorem}

\begin{definition}
  $l(\C(u))$ is the length of the code word $\C(u)$ and $l(\C)$ is the expected length of the code:
  \begin{align*}
    l(\C) = \sum_u l(\C(u)) p(u)
  \end{align*}
\end{definition}

\begin{definition}[Kraft sum]
  Given $\C : \U \rightarrow \{ 0, 1\}^*$
	\begin{align*}
		kraftsum(\C) = \sum_u 2^{-l(\C(u))}
	\end{align*}
\end{definition}

\begin{lemma}
	if $\C : \U \rightarrow \{ 0, 1\}^*$ and $\D : \V \rightarrow \{ 0, 1\}^*$
	then
  $$kraftsum(\C \times \D) = kraftsum(\C) \times kraftsum(\D)$$
	\begin{proof}
    \begin{align*}
      kraftsum(\C \times \D) &= \sum_{u, v} 2^{-(l(\C) * l(\D))} \\
      &= \sum_u 2^{-l(\C)} \sum_v 2^{- l(\D)}
    \end{align*}
	\end{proof}
\end{lemma}

\begin{corollary}
  $kraftsum(\C^n) = (kraftsum(\C))^n$
\end{corollary}

\begin{proposition}
  if $\C$ is non-singular, then
  \begin{align*}
    kraftsum(\C) \leq 1 + \max_u l(\C(u))
  \end{align*}
\end{proposition}

In coding theory, the \textbf{Kraft-McMillan inequality} gives a necessary and sufficient condition for the existence of a uniquely decodable code for a given set of codeword lengths.

\begin{wikipedia}
  Kraft's inequality limits the lengths of codewords in a prefix code: if one takes an exponential of the length of each valid codeword, the resulting set of values must look like a probability mass function, that is, it must have total measure less than or equal to one. Kraft's inequality can be thought of in terms of a constrained budget to be spent on codewords, with shorter codewords being more expensive.
\end{wikipedia}

\begin{theorem}
  if $\C$ is uniquely decodable, then $kraftsum(\C) \leq 1$
\end{theorem}

\begin{proof}
  $\C$ is uniquely decodable $\equiv$ $\C^*$ is non singular
  \begin{align*}
    &\Rightarrow kraftsum(\C^n) \leq 1 + \max _{u_1, ..., u_n} l(\C^n) \\
    &\Rightarrow kraftsum(\C)^n \leq 1 + n L, \quad L = \max l(\C(n))
  \end{align*}
  A growing exp cannot be bounded by a linear function
  \begin{align*}
    \Rightarrow kraftsum(\C) \leq 1
  \end{align*}
\end{proof}

\begin{theorem}
  Suppose $\C : \U \rightarrow \N$ is such that $\sum_u 2^{-l(\C(u))} \leq 1$, then, there exists a prefix-free code $\C': \U \rightarrow \{0, 1\}$ s.t. $\forall u, l(\C'(u)) = l(\C(u))$
\end{theorem}

\begin{proof}
  Let $\U = \{u_1, ..., u_n\}$ and $\C(u_1) \leq \C(u_2) \leq ... \leq \C(u_k) = \C_{max}$.
  Consider the complete binary tree up to depth $\C_{max}$ initially all nodes are available to be used as codewords.
  For $i = 1, 2, ..., n$, place $\C(u_i)$ at an available node at level $\C(u_i)$ remove all descendant of $\C(u_i)$ from the available list.

  \begin{corollary}
    Suppose $\C: \U \rightarrow \{0, 1\}^*$ is uniquely decodable, then there exist an $\C': \U \rightarrow \{0, 1\}^*$ which is prefix-free and $l(\C'(n)) = l(\C(n))$
  \end{corollary}
\end{proof}

\begin{example}
  $\U = \{a, b, c, d\}$, $\C: \{0, 01, 011, 111\}$ and $\C': \{0, 10, 110, 111\}$\\
  In this case, decoding $\C$ may require delay, while decoding $\C'$ is instanteneous.
\end{example}

% -------------------------------------------------------------
\newpage
\section{Alphabet with statistics}

Suppose we have an alphabet $\U$, and suppose we have a random variable $U$ taking values in $\U$. We denote by $p(u) = Pr(U = u), u \in \U$ with $p(u) \geq 0$ and $\sum_u p(u) = 1$.\\

Suppose we have a code $\C: \U \rightarrow \{0, 1\}^*$. We then have $\C(u)$ a random binary string and $l(\C(u))$ a random integer.

\begin{example}
  $\U = \{a, b, c, d\}$\\
  $p: \{0.5, 0.25, 0.125, 0.125\}$ \\
  $\C: \{0, 01, 110, 111\}$ \\

  then we have
  \begin{align*}
    l(\C(u)) =
    \left\{
      \begin{array}{l}
        1, \quad p = 0.5 \\
        2, \quad p = 0.25 \\
        3, \quad p = 0.125 + 0.125 + 0.25
      \end{array}
    \right.
  \end{align*}
\end{example}

We can measure how efficient $\C$ represents $\U$ by considering
\begin{align*}
  E[l(\C(u))] = \sum_u p(u)l(\C(u)) \quad \text{with} \quad \C(u) = l(\C(u))
\end{align*}

\begin{theorem}
  if $\C$ is uniquely decodable, then
  \begin{align*}
    E[l(\C(u))] \geq \sum_u p(u) \log(\frac{1}{p(u)})
  \end{align*}
\end{theorem}

\begin{proof}
  let $\C(u) = l(\C(u))$, we know $\sum_u 2^{-\C(u)} \leq 1$ because $\C$ is uniquely decodable. We write $q(u)~=~2^{-\C(u)}$ and get

  \begin{align*}
    E[l(\C(u))] &= \sum_u p(u) \C(u) = \sum_u p(u) \log_2\frac{1}{q(u)} \\
    &\equiv \sum_u p(u) \log\frac{q(u)}{p(u)} \leq 0 \\
    &\equiv \sum_u p(u) \ln\frac{q(u)}{p(u)} \leq 0 \\
    &\leq \sum_u p(u) \left[\frac{q(u)}{p(u)} - 1\right]
    = \underbrace{\sum_u q(u)}_{\leq 1} - \underbrace{\sum_u p(u)}_{=1} \leq 0
  \end{align*}
\end{proof}

\begin{theorem}
  For any $\U$, there exists a prefix-free code $\C$ s.t.
  \begin{align*}
    E[l(\C(u))] < 1 + \sum_{u \in \U} p(u) \log\frac{1}{p(u)}
  \end{align*}
\end{theorem}
\begin{proof}
  Given $\U$, let
  \begin{align*}
    &\C(u) = \lceil \log\frac{1}{p(u)} \rceil < 1 + \log \frac{1}{p(u)} \\
    &\Rightarrow \sum_u 2^{-\C(u)} \leq \sum_u p(u) = 1 \\
    &\Rightarrow \sum_u p(u) \C(u) < \sum_u p(u) \log(\frac{1}{p(u)}) + \underbrace{1}_{\sum p(u)}
  \end{align*}
\end{proof}

\begin{definition}[Entropy]
  Entropy quantifies the amount of uncertainty involved in the value of a random variable or the outcome of a random process.
\end{definition}

\begin{theorem}
  The entropy of a random variable $U \in \U$ is
  \begin{align*}
    H(U) = \sum_{u \in \U} p(u) \log(\frac{1}{p(u)})
  \end{align*}
  with $p(u) = Pr(U = u)$
\end{theorem}

\begin{wikipedia}
  The entropy is a lower bound on the optimal expected length
  \begin{align*}
    H(U) \leq \E{l(\C(u))}
  \end{align*}
  In fact, one can show that there exists a uniquely decodable code such that
  \begin{align*}
     H(U) \leq \E{l(\C(u))} < H(U) + 1
   \end{align*}
\end{wikipedia}

Note that $H(U)$ is a fonction of the distribution $\C_u(.)$ of the random variable $U$, it isn't a function of $U$.

\begin{align*}
  H(U) = E[f(U)] \quad \text{where} \quad f(U) = \log(\frac{1}{p(u)})
\end{align*}

How to design optimal codes (in the sense of minimizing $E[l(\C(u))]$)? \\
Formally, given a random variable $U$, find $\C(u) \rightarrow \N$ s.t.
\begin{align*}
  \sum_{u \in U} 2^{\C(u)} \leq 1
\quad \text{that minimizes} \quad
  \sum_{u \in U} p(u)\C(u)
\end{align*}

Properties of optimal prefix-free codes
\begin{itemize}
  \item if $p(u) < p(v)$ then $\C(u) \geq \C(v)$
  \item The two longest codewords have the same length
  \item There is an optimal code such that the two least probable letters are assigned codewords that differ in the last bit.
\end{itemize}

Observe that if $\{\C(u_1), ... , \C(u_{k-1}), \C(u_k)\}$ is a prefix-free collection of the property that
\begin{align*}
\left.
\begin{array}{l l}
  \C(u_{k-1}) &= \alpha 0 \\
  \C(u_k)     &= \alpha 1
\end{array}
\right.
\quad \text{with} \quad \alpha \in \{ 0, 1\}^*
\end{align*}

then $\{\C(u_1), ..., \C(u_{k-2}), \alpha\}$ is also a prefix-free collection.

Also
\begin{align*}
  \sum_{u \in \U} p(u) l(\C(u)) &= p(u_1) l(\C(u_1)) + ... +  p(u_{k-2}) l(\C(u_{k-2}))
  + [p(u_{k-1}) + p(u_k)](l(\alpha) + 1) \\
  &= (p(u_{k-1}) + p(u_k)) + \sum_{v \in \V} p(v) l(\C'(v))
\end{align*}

So we have shown that with
\begin{align*}
  E[l(\C(U)] = p(u_{k-1}) + p(u_k) + E[l(\C'(V))]
\end{align*}
if $\C$ is optimal for $U$, then $\C'$ is optimal for $V$

% -------------------------------------------------------------
\newpage
\section{Entropy and mutual information}
\label{sec:entropy}

\begin{definition}[Joint entropy]
  Suppose $U, V$ are random variables with $p(u,v) = \Pr{U=u, V=v}$, the joint entropy is
  \[
    H(UV) = \sum_{u,v} p(u,v) \log \frac 1 {p(u,v)}
  \]
\end{definition}

\begin{theorem}
  \[
    H(UV) \leq H(U) + H(V)
  \]
  with equality iff $U$ and $V$ are independants.
\end{theorem}

\begin{proof}
  We want to show that
  \begin{align*}
    \sum_{u,v} p(u,v) \log \frac 1 {p(u,v)} \leq \sum_u p(u) \log \frac 1 {p(u)} + \sum_v p(v) \log \frac 1 {p(v)}
    \iff \sum_{u,v} p(u,v) \log \frac {p(u)p(v)} {p(u,v)} \leq 0
  \end{align*}
  We use $\ln z \leq z - 1$ for all $z$ (with equality iff $z=1$):
  \begin{align*}
    \sum_{u,v} p(u,v) \log \frac {p(u)p(v)} {p(u,v)} \leq \sum_{u,v} p(u,v) \left[ \frac {p(u)p(v)} {p(u, v)} - 1 \right] = \sum_{u,v} p(u)p(v) - \sum_{u,v} p(u,v) = 1 - 1 = 0
  \end{align*}
\end{proof}

Same definitions of entropy holds for $n$ symbols.

\begin{definition}[Joint Entropy]
  Suppose $U_1, U_2, \dots, U_n$ are RVs and we are given $p(u_1 \dots u_n)$, the joint entropy is
  \[
    H(U_1, \dots, U_n) = \sum_{u_1 \dots u_n} p(u_1 \dots u_n) \log \frac 1 {p(u_1 \dots u_n)}
  \]
\end{definition}

\begin{theorem}
  \[
    H(U_1 \dots U_n) \leq \sum_{i=1}^n H(U_i)
  \]
  with equality iff $U$s are independants
\end{theorem}

\begin{corollary}
  if $U_1, \dots, U_n$ are i.i.d. then $H(U_1 \dots U_n) = nH(U_1)$
\end{corollary}

\begin{definition}[Conditional entropy]
  \begin{align*}
    H(U|V) &= \sum_{u,v} p(u,v) \log \frac 1 {p(u|v)}\\
           &= \sum_v H(U | V = v) \Pr{V = v}
  \end{align*}
\end{definition}

\begin{theorem}
  \[
    H(UV) = H(U) + H(V|U) = H(V) + H(U|V)
  \]
\end{theorem}

\begin{theorem}
  \[
    H(U) + H(V) \geq H(UV) = H(V) + H(U|V)
  \]
\end{theorem}

\begin{definition}[Mutual information]
  Mutual information measures the amount of information that can be obtained about one random variable by observing another.
  \begin{align*}
  I(U;V) = I(V;U) &= H(U) - H(U|V)\\
  &= H(V) - H(V|U)\\
  &= H(U) + H(V) - H(UV)
  \end{align*}
\end{definition}

We can apply the chain rule on the entropy as follow

\[
  H(U_1 U_2 \dots U_n) = H(U_1) + H(U_2|U_1) + \dots + H(U_n|U_1 U_2 \dots U_{n-1})
  = \sum_{i = 1}^n H(U_i | U^{i-1})
\]

\begin{definition}[Conditional mutual information]
  \begin{align*}
    I(U;V|W) &= H(U|W) - H(U|VW)\\
    &= H(V|W) - H(V|UW)\\
    &= \E_{u,v,w} \left[ \log \frac {p(uv|w)} {p(u|w)p(v|w)} \right]
  \end{align*}
\end{definition}

\begin{theorem}
  \[
    I(V;U_1\dots U_n) = I(V;U_1) + I(V;U_2 | U_1) + \dots + I(V;U_n|U_1 \dots U_{n-1})
  \]
\end{theorem}

We can apply the chain rule on the mutual information as follows
\[
  I(U_1, U_2, ... ; V) = I(U_1; V) + I(U_2; V | U_1) + ...
\]

\begin{theorem}{Data processing inequality}
  Let $X \rightarrow Y \rightarrow Z$ be a Markov chain, then
  \begin{align*}
    I(X ; Y) \geq I(X ; Z)
  \end{align*}
\end{theorem}

\begin{notation}
  \[
    U^n \triangleq (U_1 U_2 \dots U_n)
  \]
\end{notation}

\begin{theorem}
  \[
    I(U;V |W) \geq 0
  \]
  equality iff conditioned on $w$, $u$ and $v$ are independant, that is iff $U-V-W$ is a Markov chain.
\end{theorem}


\begin{proof}
  \begin{align*}
    I(U;V|W) &= \frac 1 {\ln 2} \sum_{u,v,w} p(uvw) \ln \frac {p(u|w)p(v|w)} {p(uv | w)}\\
    &\geq \frac 1 {\ln 2} \sum_{u,v,w} p(uvw) \left[ \frac {p(u|w)p(v|w)} {p(uv | w)} - 1 \right]\\
    &=\frac 1 {\ln 2} \sum_{u,v,w}(p(w)p(u|w)p(v|w) - p(uvw))\\
    &= \frac 1 {\ln 2}(1 - 1) \\
    &=0
  \end{align*}
\end{proof}

% -----------------------------------------------------------------------
\newpage
\section{Data processing}

\begin{theorem}
  $U-V-W$ is a MC $\iff I(U;W|V) = 0$
\end{theorem}

\begin{corollary}
  $I(U;V) \geq I(U;W)$ and by symetry of MC $I(W;V) \geq I(U;W)$
\end{corollary}

\begin{proof}
  \[
    I(U;VW) = I(U;V) + I(U;W|V) = I(U;V)
  \]
  and
  \[
    I(U;VW) = I(U;W) + I(U;V|W) \geq I(U;W)
  \]
\end{proof}

\begin{theorem}
  Given $U$ a RV taking values in $\U$ then $0 \leq H(U) \leq \log | \U |$. $H(U)=0$ iff $U$ is constant, $H(U)=\log | \U |$ iff $U$ is $p(u) = 1 / |\U|$ for all $u$.
\end{theorem}

\begin{proof}
For the lower bound,
  \[
    H(U) = \sum_u \underbrace{p(u)}_{\geq 0} \underbrace{\log \frac 1 {p(u)}}_{\geq 0} \geq 0
  \]
For the upper bound,

\begin{align*}
  H(U) - \log | \U |
  &= \sum_u p(u) \log \frac 1 {p(u)} - \sum_u p(u) \log |\U|\\
  &= \frac 1 {\ln 2} \sum_u p(u) \ln \frac 1 {|\U | p(u)}\\
  &\leq \frac 1 {\ln 2} \sum_u p(u) \left(\frac 1 {|\U | p(u)} - 1 \right)\\
  &=\frac 1 {\ln 2} \left[ \sum_u \frac 1 {|\U |} - \sum_u p(u) \right]\\
  &=0
\end{align*}
\end{proof}

\begin{theorem}
  $I(U;V) = 0 \iff U \bot V$
\end{theorem}


\begin{definition}[Entropy rate of a stochastic process]
\[
  r = \lim_{n\to \infty} \frac 1 n H(U^n) \quad \text{ if the limit exists}
\]
\end{definition}

\begin{theorem}
  For stationary stochastic process $U^n$, the sequences
  \[
    a_n = \frac 1 n H(U^n) \text{ and } b_n = H(U_n|U^{n-1})
  \]
  are positive and non increasing. Then $a=\lim_{n\to \infty} a_n$ and $b=\lim_{n\to \infty} b_n$ exists and $a=b$.
\end{theorem}

\begin{proof}
\begin{align*}
  b_{n+1}
  &= H(U_{n+1}|U_1, U_2,\dots,U_n)\\
  &\leq H(U_{n+1}|U_2,\dots,U_n)\\
  &= H(U_n|U_1, U_2,\dots,U_{n-1})\\
  &= b_n \text{ , because $U_1 \dots U_n \sim U_2 \dots U_{n+1}$ (Stationarity).}
\end{align*}


Hence, it is non-increasing.\\\\

For the \{$a_n$\}, observe that

\begin{align*}
a_n = \frac{1}{n} H(U^n)
&= \frac{1}{n} \bigg[ H(U_1)+H(U_2|U_1)+H(U_3|U^2) +\dots+H(U_n|U^{n-1}) \bigg]\\
&= \frac{1}{n} \bigg[ b_1+b_2+\dots+b_n\bigg]
\end{align*}


and by the "Lemma", whenever $b_n \rightarrow b$ , \space $a_n \rightarrow b$
\end{proof}
\begin{lemma}[Cesaro]
	Suppose $b_n \rightarrow b$, \\\\

	then,
	\[
	a_n = \frac{1}{n} \bigg[ b_1+b_2+\dots+b_n\bigg] \text{ also converges and to 1.}
	\]
\end{lemma}
\begin{proof}
	Since $b_n \rightarrow b$ , $\bigg( \equiv \forall \epsilon > 0$ , $\exists$ $n(\epsilon)$ s.t $ \forall n > n(\epsilon)$  $ |b_n-b| < \epsilon\bigg)$\\

	$\exists B $ s.t. $ |b_n| < B$ for all n.\\\\
	Take $n > n_1(\epsilon) \triangleq \dots $ then
	\[
		|a_n-b| \leq \frac{|b_1-b|+|b_2-b|+|b_3-b|+\dots+|b_n-b|}{n}
	\]
	\[
		\text{so  }  |a_n-b| \leq \frac{1}{n}\bigg[ \sum_{i=1}^{n_0(\epsilon)} \underbrace{|b_i-b|}_{2B}  + \sum_{i=n_0(\epsilon)+1}^{n} \underbrace{|b_i-b|}_{\leq \epsilon}\bigg] \leq \frac{n_0(\epsilon) 2B}{n} + \epsilon < 2\epsilon
	\]
	\[
		\text{for } n>n_1(\epsilon) \triangleq \text{max, } \{ n_0(\epsilon) \frac{1}{\epsilon} n_0(\epsilon) 2B \}
	\]
\end{proof}



% -------------------------------------------------------------------------
% class 9.10.2017

\begin{theorem}
  Given a stationary process with entropy rate $r$:
  \begin{align*}
    r = \lim_{n \rightarrow \infty} \frac{1}{n} H(U^n)
  \end{align*}

  then
  \begin{enumerate}
    \item for every source coding scheme
    \begin{align*}
      \C_n: U^n \rightarrow \{0, 1\}^*
    \end{align*}
    the expected number of bits / letter is given by
    \begin{align*}
      \frac{1}{n} E[l(\C(U^n))] \geq r
    \end{align*}
    \item for any $\epsilon > 0$, there exists a source coding scheme $\C_n: U^n \rightarrow \{0, 1\}^*$ s.t.
    \begin{align*}
      \frac{1}{n} E[l(\C_n(U^n))] < r + \epsilon
    \end{align*}
  \end{enumerate}
\end{theorem}

\begin{proof}
  \begin{enumerate}
    \item we already know
    \begin{align*}
      \frac{1}{n} E[l(\C_n(U^n))] \geq \frac{1}{n} H(U^n)
    \end{align*}
    and the right term is decreasing
    \item we also know that for each $n, \exists \C_n$ that is prefix-free s.t.
    \begin{align*}
      E[l(\C_n(U^n))] < \underbrace{\frac{1}{n} H(U^n)}_{r} + \underbrace{\frac{1}{n}}_{0}
    \end{align*}
    we can find $n$ large enough s.t. the right hand side $< r + \epsilon$
  \end{enumerate}
\end{proof}

% -------------------------------------------------------------------------
% -------------------------------------------------------------------------

\section{Typicality and typical set}

Suppose we have a sequence $U_1, U_2, ...$ of i.i.d. random variables taking values in an alphabet $\U$.
Suppose we observe $u_1,u_2..., u_n$. We will call it to be \textit{typical-$(\epsilon, p)$} if
\begin{align*}
  p(u) (1 - \epsilon)
  \leq \frac{\# \text{ of times $u$ appears in $u_1, ..., u_n$}}{n}
  \leq p(u)(1+\epsilon)
\end{align*}

\begin{theorem}
  $u^n$ is $(\epsilon, p)$-typical then
  \begin{align*}
    2^{-n H(u)(1 + \epsilon)}
    \leq Pr(U^n = u^n)
    \leq 2^{-n H(u)(1 - \epsilon)}
  \end{align*}
\end{theorem}

\begin{proof}
  \begin{align*}
    Pr(U^n = u^n) &= \prod_{i=1}^n Pr(U_i = u_i) = \prod_{i=1}^n p(u_i) = \prod_{u \in \U} p(u)^{\#_u}
  \end{align*}
  with $\#_u$ the number of times $u$ appears in $u_1, ..., u_n$ where
  \begin{align*}
    n (1-\epsilon) p(u) \leq \#_u \leq n(1+\epsilon)p(u)
  \end{align*}
  consequently
  \begin{align*}
    p(u)^{(n p(u)(1-\epsilon))} \geq p(u)^{\#_u} \geq p(u)^{n p(u)(1+\epsilon)}
  \end{align*}
  then
  \begin{align*}
    (\prod_{n} p(u)^{p(u)})^{(1-\epsilon)n}
    \geq Pr(U^n = u^n)
    \geq (\prod_{n} p(u)^{p(u)})^{(1+\epsilon)n}
  \end{align*}
  but
  \begin{align*}
    p(u)^{p(u)} = 2^{-p(u) \log(\frac{1}{p(u)})} \Rightarrow \prod p(u)^{p(u)} = 2^{-H(u)}
  \end{align*}
\end{proof}

\begin{definition}[Typical set]
  \begin{align*}
    T(n, \epsilon, p) = \{ u^n \in \U^n : u^n \text{ is } (\epsilon, p)\text{-typical}\}
  \end{align*}
\end{definition}

\begin{wikipedia}
  Typical sets provide a theoretical means for compressing data, allowing us to represent any sequence $X^n$ using $nH(X)$ bits on average, and, hence, justifying the use of entropy as a measure of information from a source.
\end{wikipedia}

\begin{theorem}
  \begin{enumerate}
    \item if $u^n \in T(n, \epsilon, p)$ then
    \begin{align*}
      p(u^n) = Pr(U^n = u^n) = 2^{-n H(u)(1 \pm \epsilon)}
    \end{align*}
    when $U_i$ i.i.d.
    \item
    \begin{align*}
      \lim_{n \rightarrow \infty} Pr(U^n \in T(n, \epsilon, p)) = 1
    \end{align*}
    \item
    \begin{align*}
      |T(n, \epsilon, p)| \leq 2^{n (H(u)(1 + \epsilon))}
    \end{align*}
    \item
    \begin{align*}
      |T(n, \epsilon, p)| \geq (1-\epsilon) 2^{n H(u)(1-\epsilon)}
    \end{align*}
  \end{enumerate}
\end{theorem}

\begin{proof}
  \begin{enumerate}
    \item
    Fix $u \in \U$ let $X_i = 1$ if $ U_i = u$ and $0$ otherwise
    \begin{align*}
      \frac{\text{\# of times } u \text{ appears in } U_1 ... U_n}{n} = \frac{1}{n} \sum_{i = 1}^n X_i
    \end{align*}
    observe that $\{ X_i \}$ are i.i.d.
    \begin{align*}
      &X_i =
      \left\{
        \begin{array}{l l}
          1 & \text{ w.p. } p(u) \\
          0 & \text{ w.p. } 1 - p(u)
        \end{array}
      \right. \\
      &\Rightarrow \Ex{X_i} = p(u) \quad \text{ and } \quad Var[X_i] = p(u) - p(u)^2 \\
    \end{align*}
    \begin{align*}
      \underbrace{
        \Pr{|\frac{1}{n} \sum_{i+1}^n X_i - p(u)|}\geq \epsilon p(u)
      }_{u^n \text{ fails the test for letter } u}
      \leq
      \frac{Var(\frac{1}{n} \sum X_i)}{(\epsilon p(u))^2}
      = \frac{(1 - p(u))}{\epsilon^2 p(u)}
    \end{align*}

    \item
    \begin{align*}
      \Pr{U^n \not \in T(n, \epsilon, p)} &= \Pr{\bigcup_{u \in U} \{u^n \text{ fails the test for u} \}} \\
      &\leq \sum_{u \in U} \Pr{U^n \text{ fails the test for } u} \\
      &\leq \frac{1}{n} \sum_{u \in U} \frac{(1 - p(u))}{p(u) \epsilon^2} \quad \text{ which goes to 0 as } n \text{ gets large}
    \end{align*}

    \item
    \begin{align*}
      1 \geq \Pr{U^n \in T(n, \epsilon, p)} &= \sum_{u^n \in T(n, \epsilon, p)} \Pr{U^n = u^n} \\
      &\geq \sum_{u^n \in T(n, \epsilon, p)} 2^{-n(1 + \epsilon) H(u)} \\
      &= 2^{-n (1 + \epsilon) H(u)} |T(n, \epsilon, p)|
    \end{align*}

    \item
    \begin{align*}
      1 - \epsilon \leq \Pr{U^n \in T(n, \epsilon, p)} &= \sum_{u^n \in T(n, \epsilon, p)} \Pr{U^n = u^n} \\
       &\leq \sum_{u^n \in T(n, \epsilon, p)} 2^{n H(u)(1-\epsilon)} \\
       &= 2^{-n H(u)(1 - \epsilon)} |T(n, \epsilon, p)|
    \end{align*}

  \end{enumerate}
\end{proof}

\begin{observation}
  $\Pr{U^n \in T(n, \epsilon, p)} \to 1$ as $n\to \infty$
\end{observation}

\begin{definition}[Kullback Leibler divergence]
  \[
    D(p||q) = \sum_u p(u) \log \frac {p(u)} {q(u)} \geq 0 \text{ with equality iff } p = q
  \]
\end{definition}

If we compress data in a manner that assumes $q(u)$ is the distribution underlying some data, when, in reality, $p(u)$ is the correct distribution, the Kullback-Leiber divergence is the average number of additional bits per datum necessary for compression. It is also called \textbf{relative entropy} and is a measure of how one probability distribution diverges from a second probability distribution.

% ----------------------------- %
% ---- lecture 2017-10-10 ----- %
% ------------ jb ------------- %
% ----------------------------- %

\begin{lemma}
  if $U_1 \dots U_n$ are i.i.d. with distribution $q$ and $u_1 \dots u_n$ is $(\epsilon, p)$-tipycal, then
  \[
    2^{-n[H(p) + D(p||q)](1+\epsilon)}
    \leq
    \Pr{U^n = u^n}
  \leq
    2^{-n[H(p) + D(p||q)](1-\epsilon)}
  \]
\end{lemma}

\begin{proof}
  Follows from
\[
  \left[ \prod_u q(u)^{p(u)} \right]^{n(1+\epsilon)} \leq \Pr{U^n = u^n} \leq  \left[ \prod_u q(u)^{p(u)} \right]^{n(1-\epsilon)}
\]
\[
  \prod_u q(u)^{p(u)} = 2^{-\sum p(u) \log \frac 1 {q(u)}}
\]
and

\[
  \sum_u p(u) \log \frac 1 {q(u)} =
  \underbrace{\sum_u p(u) \log \frac 1 {p(u)}}_{H(p)} +
  \underbrace{\sum_u p(u) \log \frac {p(u)} {q(u)}}_{D(p||q)}
\]
\end{proof}


\begin{corollary}
  if $U_1 \dots U_n$ are i.i.d. following distribution $q$, then
  \[
    2^{-n[(1+\epsilon)D(p||q)+2\epsilon H(p)]}
    \leq
    \Pr{U^n \in T(n,\epsilon,p)}
    \leq
    2^{-n[(1-\epsilon)D(p||q) - 2\epsilon H(p)]}
  \]
\end{corollary}

\begin{proof}
  \[
    \Pr{U^n \in T(n,\epsilon,p)} = \sum_{u^n \in T(n, \epsilon, p)} \Pr{U^n = u^n}
  \]
  We have
  \begin{align*}
    2^{-n[H(p) + D(p||q)](1+\epsilon)}
    \leq
    &\Pr{U^n = u^n}
    \leq
    2^{-n[H(p) + D(p||q)](1-\epsilon)}\\
    2^{nH(p)(1-\epsilon)}
    \leq
    &|T(n, \epsilon, p)|
    \leq
    2^{nH(p)(1+\epsilon)}
  \end{align*}
\end{proof}

\begin{example}
  $U \in \{0,1\}$, $p=\frac 1 2, \frac 1 2$, $q=\frac 1 2 - \delta, \frac 1 2 + \delta$
  \[
    D(p||q) = \frac 1 2 \log \frac 1 {1-2\delta} + \frac 1 2 \log \frac 1 {1+2\delta} = \frac 1 2 \log \frac 1 {1-4\delta^2} = - \frac 1 2 \log (1-4\delta^2) \approx \frac 1 2 4\delta^2 + o(\delta^4)
  \]
  So if we want $2^{-n D(p||q)}$ small, we must pick $n=\Omega(1/\delta^2)$
\end{example}

\begin{example}
  Suppose we are told that $U$ is $p$ distributed and $p(u)$ are powers of 2. We design a prefix-free code $\C$ to minimize $\sum_u p(u) l(\C(u))$. We have been misinformed and $U\sim q$, then:

  \begin{align*}
    \Ex{l(\C(u))}
    &= \sum_u q(u) \log \frac 1 {p(u)}\\
    &= \underbrace{H(q)}_{\text{length for optimal code}} + \underbrace{D(q||p)}_{\text{penalty for misbelief}}
  \end{align*}
\end{example}

\subsection{Universal data compression}

Suppose we know that the distribution $p$ of $U$ is either $p_1$, $p_2$ ... $p_k$, can we design a code $\C: U \to \{0,1\}^*$

\[
  \Ex{l(\C(U))} \leq H(U) + \text{small for every } p
\]

\[
  \Ex{\frac 1 n l(\C(U))} \leq o(n) + \Ex{h_2 \pfrac K n}
\]
with $K = \sum_{i=1}^n u_i$

We have $\frac {\Ex{K}} n = \theta_1$ and $\Ex{h_2\pfrac K n} \leq h_2 \left(\Ex{\frac K n} \right) = h_2(\theta)$

\paragraph{Design $\C$}
Because the probability of a bit string is only dependant of the number of
1s (or 0s), it makes sense to encode two strings with the same numbers of 1
with code words of same lengths.
Given $u_1 \dots u_n \in \{0,1\}^n$, first count the number of 1, call it $k$.


\[
  \C(u_1 \dots u_n) =
  \underbrace{\text{describe } k}_{\lceil \log (n+1)\rceil}
  \underbrace{\text{describe } u_1 \dots u_n}_{\lceil \log {n \choose k}\rceil}
\]

We now want to evaluate

\[
  \frac 1 n \Ex{l(\C(U))}
\]

when $U_1 \dots U_n$ are i.i.d with $p_1 = \theta$ and $p_0 = 1 - p_1$

\begin{observation}

for any $0 \leq \alpha \leq 1$

\begin{align*}
  1 = 1^n = (\alpha + (1-\alpha))^n &= \sum_{i=0}^n {n \choose i} \alpha^i (1-\alpha)^{n-i}\\
  & \geq {n \choose k} \alpha^k (1-\alpha)^{n-k}
\end{align*}

Then for all $\alpha$

\[
{n \choose k} \leq \alpha^{-k}(1-\alpha)^{-(n-k)} = 2^{n(\frac k n \log \frac 1 \alpha + (1-\frac k n) \log \frac 1 {1-\alpha})}
\]

We pick $\alpha = \frac k n$, and we get

\[
  {n \choose k} < 2^{n h_2 \pfrac k n}
\]

\end{observation}

Using this bound we have

\[
  \frac 1 n l(\C(u_1 \dots u_n)) \leq \frac 2 n + \frac {\log (n+1)} n + h_2\pfrac k n
\]

\[
  \Ex{\frac 1 n l(\C(U))} \leq o(n) + \Ex{h_2 \pfrac k n}
\]

\begin{claim}
Suppose $U_i$ are i.i.d. with $\Pr{U_1=1}=\theta$. We have $\Ex{ \frac k n} = \theta$ and $\Ex{h_2 \pfrac k n } \leq h_2(\Ex{\frac k n}) = h_2(\theta)$. So
\[
  \lim_{n\to \infty} \frac 1 n \Ex{l(\C(u_1\dots u_n))} \leq h_2(\theta)
\]
consequently this scheme is asymptotically optimal.
\end{claim}

\begin{proof}
  To prove the claim we need to show that if $\beta_1\dots \beta_k$ are in $[0,1]$ and $q_1 \dots q_k$ are non negative numbers that sum to 1 then

  \[
    \sum_{i=1}^k q_i h_2(\beta_i) \leq h_2 \left(\sum_{i=1}^k q_i \beta_i \right)
  \]

  Let $U$ and $V$ be random variables with $U \in \{0,1\}$ and $V \in \{1,\dots,k\}$ with

  \begin{align*}
    \Pr{V=i} &= q_i\\
    \Pr{U=1|V=i} &= \beta_i\\
    \Pr{U=0|V=i} &= 1- \beta_i\\
  \end{align*}
  Then,
  \begin{align*}
    \Pr{U=1} &= \sum_i q_i \beta_i\\
    H(U) &= h_2\left(\sum_i q_i \beta_i \right)\\
    H(U|V) &= \sum_i q_i h_2(\beta_i)
  \end{align*}

  And we already know that $H(U) \geq H(U|V)$
\end{proof}



% ----------------------------- %
% ---- lecture 2017-10-16 ----- %
% --------- thomas ------------ %
% ----------------------------- %









% -------------------------------------------------------------------------
% lesson - 16.0.2017 - Thomas

\todo{Thomas scribes here}

% -------------------------------------------------------------------------
% lesson - 17.0.2017 - sebastien

Suppose we have an infinite string $u_1 u_2 ..., u_i \in \U$, and
$$u_1 u_2 ... = v_1 v_2 ... \text{ with } v_i \in \U^*, v_i \neq v_j \text{ when } i \neq j$$
for any $k$ we have
\begin{align*}
  \lim_{m \to \infty} \frac{length(v_1...v_m)}{m} \geq k
  \Rightarrow \lim_{m \to \infty} \frac{length(v_1 ... v_m)}{m} =
  \infty
\end{align*}

\begin{definition}
  Given an infinite string $u_1 u_2 ...$ and a machine $M$, let
  \begin{align*}
    \rho_{M}(u_1 u_2 ...) = \overline{\lim_{n \to \infty}} \frac{\text{length of the output } M \text{ after reading } u_1 u_2...}{n}
  \end{align*}
  also given $s > 0$, define
  \begin{itemize}
    \item The compressibility of $\U^*$ be s-state machines
    \begin{align*}
      \rho_s (u_1 u_2 ...) = \min_{M} \rho_{M}(u_1 u_2 ...)
    \end{align*}
    with $M$ an $s'$-state machine with $s' \leq s$
    \item Compressibility of $\U^*$ by finite state machines
    \begin{align*}
      \rho_{FSM} (u_1 u_2 ...) = \lim_{s \to \infty} \rho_s (u_1 u_2 ...)
    \end{align*}
  \end{itemize}
\end{definition}

\begin{definition}
  Suppose $u_1 u_2 ...$ an infinite sequence, define $m(n)$ as the largest $m$ for which $u_1 ... u_n = v_1 ... v_m$ with distinct $v_1 ... v_m$
  \begin{example}
    $$ u = aaaaaaaaa, \quad \underbrace{\emptyset}_{v_1} \underbrace{a}_{v_2} \underbrace{aa}_{v_3} \underbrace{aaa}_{v_4} \underbrace{aaaa}_{v_5} \quad \Rightarrow m(10) = 5 $$
  \end{example}
\end{definition}

So far we know that

\begin{align*}
  \frac{\text{ length of the output of any s-state IL machine when it reads } u_1 u_2 ... }{n} \geq \frac{m(n) \log(\frac{m(n)}{8 s^2})}{n}
\end{align*}
with
\begin{align*}
  \frac{m(n) \log(\frac{m(n)}{8 s^2})}{n} = \frac{m(n) \log(m(n))}{n} - \frac{m(n) \log(8 s^2)}{length(v_1 ... v_m)}
\end{align*}

hence if $M$ is a s-state machine
\begin{align*}
  \rho_M (u_1 u_2 ...) \geq \overline{\lim_{n \to \infty}} \frac{m(n) \log(m(n))}{n}
  \quad \text{ then } \quad
  \rho_{FSM} (u_1 u_2 ...) \geq \overline{\lim_{n \to \infty}} \frac{m(n) \log m(n)}{n}
\end{align*}


% -------------------------------------------------------------------------
% Lemple-Ziv

\newpage
\section{Lemple-Ziv data compression method}

Given some alphabet $\U$ to both encoder and decoder, they also agree an order on $\U$:

\begin{enumerate}
  \item Start with a dictionary $\D = \U$
  \item To each word $w \in \D$, assign a $\lceil \log |\D| \rceil $-bit binary description in the dictionary order
  \item Parse the first word $w$ in $u_1 u_2 ...$ in the dictionary, output its binary description
  \item replace $w$ in $\D$ by $\{ wu, \forall u \in \U \}$.
  \item Go to 2.
\end{enumerate}

\begin{example}
  Define an alphabet $\U = \{a, b, c\}$ with $a \leq b \leq c$ and an input message
  $$ u = a b a c a c $$
  \begin{itemize}
    \item Create the dictionary $\D = \{a, b, c\}$ and its corresponding binary description $\D_{bin} = \{00, 01, 10\}$
    \item The first word in the message is $'a'$, output its binary description
    $$ output = 01 $$
    \item Update the dictionary:
    $$ \D = \{a, ba, bb, bc , c\} \quad \D_{bin} = \{000, 001, 010, 011, 100\} $$
    \item Parse the next word $'ba'$ and output its binary description
    $$ output = 01 001 $$
    \item Update the dictionary
    $$ \D = \{a, baa, bab, bac, bb, bc , c\} \quad \D_{bin} = \{000, 001, ...\} $$
    \item Continue until the end of the input data...
  \end{itemize}
  The decoder can proceed in a similar way to iteritavely update the dictionary while decoding the message.
\end{example}

\subsection{Analysis of LZ}

Observe that LZ parses the string $u_1 u_2 ...$ into $v_1 v_2 ...$ with $v_i \in \U^*$ or $v_i \in \D_i$ where $\D_i$ is the dictionary at step $i$.

When going from iteration $i \rightarrow i+1$, $v_i$ is removed from $\D$, consequently $v_1, v_2, v_3$ are distinct.

The length of the output of LZ after reading $u_1 ... u_m$ is given by
\begin{align*}
  \text{LZ output's length} = \lceil \log |\U| \rceil + \lceil \log (2 |\U| - 1) \rceil
  + \lceil \log (3 |\U| - 2) \rceil + ...
  + \lceil \log (m|\U| - m + 1) \rceil
\end{align*}
we observe that
\begin{align*}
  \text{LZ output's length} < m(\log(m |\U|) + 1) = m \log(2 m |\U|)
\end{align*}

Also we have

\begin{align*}
  \text{\# bits / letter} &< \frac{m \log(2m |\U|)}{length(u_1 ... u_m)} \\
  &= \frac{m \log(m)}{ length(u_1 ... u_m)} + \frac{m \log(2 |\U|)}{length(u_1 ... u_m)}
\end{align*}

therefore
\begin{align*}
  \rho_{LZ}(u_1 u_2 ...) = \lim_{m \to \infty} \frac{\text{\# bits}}{\text{letter}} \leq \lim_{m \to \infty} \frac{m \log(m)}{length(u_1 ... u_m)} \leq \lim_{n \to \infty} \frac{m(n) \log(m(n))}{n} \leq \rho_{FSM}(u_1 u_2 ...)
\end{align*}

So we have proved the following theorem:

\begin{theorem}
  for every $u_1 u_2 ...$
  $$ \rho_{LZ}(u_1 u_2 ...) \leq \rho_{FSM}(u_1 u_2 ...) $$
\end{theorem}

\begin{corollary}
  if $u_1 u_2 ...$ is stationary
  $$ \rho_{LZ}(u_1 u_2 ...) = \text{ entropy rate of } u_1 u_2 ...$$
\end{corollary}


% -------------------------------------------------------------------------
% jb 2017-10-23
% -------------------------------------------------------------------------

\section{Transmission of data}

Interesting in the case of unreliable transmission media.

\begin{definition}[Communication channel]
  A communication channel $W$ is a device with an input alphabet $\X$ and an output
  alphabet $\Y$. Its behabvior is described by
  \[
    W_i(y_i | x^i, y^{i-1}) = \Pr{Y_i = y_i | X^i = x^i, Y^{i-1} = y^{i-1}}
  \]
\end{definition}

\begin{definition}[Memoryless channel]
  a channel $W$ is said to be memoryless if
  \[
      W_i(y_i | x^i, y^{i-1}) = W(y_i|x_i)
    \]
\end{definition}

\begin{definition}[Stationary channel]
  a channel $W$ is said to be stationary if
  \[
      W_i(y|x) = W(y|x)
    \]
\end{definition}

\begin{example}[Binary erasure channel - BEC]
  $\X = \{0,1\}$ and $\Y = \{0,1,?\}$, then
  \begin{align*}
    W(0|0) &= 1 - p\\
    W(?|0) &= p\\
    W(1|0) &= 0
  \end{align*}
  and same for $x_i=1$.
\end{example}

\begin{example}[Binary symetric channel - BSC]
  \begin{align*}
    W(0|0) &= 1 - p = W(1|1)&\\
    W(1|0) &= p =W(0|1)
  \end{align*}
\end{example}

The input $X_1, X_2 \dots X_n$ to a channel might have memory

\[
  \Pr{X^n = x^n} = p(x_1)p(x_2 | x_1) \dots p(x_i | x^{i-1}) \dots p(x_n|x^{n-1})
\]

\begin{align*}
  \Pr{X^n=x^n, Y^n=y^n} &= p(x_1)W_1(y_1|x_1)p(x_2|x_1,y_1)W(y_2|x_1,x_2,y_1)\dots\\
  &= \prod_i p(x_i|
  \underbrace{x^{i-1}}_{\text{feedback}}
  \underbrace{y^{i-1}}_{\text{memory}}
  )W_i(y_i|x^iy^{i-1})
\end{align*}

\begin{lemma}
  if there is no feedback and the channel is memoryless and stationary, then
  \[
    \Pr{Y^n=y^n | X^n=x^n} = \prod_{i=1}^n W(y_i|x_i)
  \]
\end{lemma}

\begin{proof}
  \begin{align*}
    \Pr{Y^n=y^n, X^n=x^n} &= \prod_{i=1}^n p(x_i|x^{i-1}y^{i-1})W_i(y_i|x^iy^{i-1})\\
    &= \prod_{i=1}^n p(x_i|x^{i-1})W(y_i|x_i)\\
    &= \prod_{i=1}^n W(y_i|x_i) \Pr{X^n=x^n}
  \end{align*}
\end{proof}

\begin{example}
  Suppose $W$ is BSC(1/2) but we have feedback, defined by $X_1=0$ and $X_i=Y_{i-1}$.

  \begin{align*}
    &\Pr{Y^2=00|X^2=01} = 0\\
    &W(0|0)W(0|1) = \frac 1 4
  \end{align*}

\end{example}

\begin{lemma}
  if $W$ is stationary memoryless and there is no feedback, then
  \[
    H(Y^n|X^n) = \sum_{i=1}^n H(Y_i|X_i)
  \]

\end{lemma}

\begin{proof}
  \[
    H(Y^n|X^n) = \Ex{\log \frac 1 {\Pr{Y^n|X^n}}}
    = \Ex{\log \prod_{i=1}^n \frac 1 {\Pr{Y_i|X_i}}}
    = \sum_{i=1}^n \Ex{\log \frac 1 {\Pr{Y_i|X_i}}}
    = \sum_{i=1}^n H(Y_i|X_i)
  \]
\end{proof}

For a memoryless stationary channel $W(Y|X)$ we can compute, for any
distribition $p(x)$, $p(x,y) = p(x)W(y|x)$ and $I(X;Y)$, we can also compute
\[
  C(W) = \max_{p(x)} I(X;Y)
\]

\begin{lemma}
  for a stationary memoryless $W$ without feedback, we have
  \[
    I(X^n;Y^n) \leq n C(W)
  \]
\end{lemma}


\begin{proof}
  \begin{align*}
    I(X^n;Y^n) &= H(Y^n) - H(Y^n|X^n)\\
    &= H(Y^n)- \sum_i H(Y_i|X_i)\\
    &\leq \sum_i H(Y_i) - \sum_i H(Y_i|X_i)\\
    &= \sum_i I(X_i;Y_i)
  \end{align*}

  Note that the joint distribution $\Pr{X_i,Y_i}$ is of the form $p(x)W(y|x)$, then $I(X_i;Y_i) \leq C(W)$
\end{proof}

\begin{notation} for simplicity
  \[
    p\ast q = (1-q)p + q(1-p)
  \]
\end{notation}

\begin{example}
  Let $W$ be a BSC($p$), $\Pr{X=0} = 1 - q$ and $\Pr{X=1} = q$. Then

  \begin{align*}
    \Pr{Y=0} = (1-q)(1-p) + qp\\
    \Pr{Y=1} = (1-q)p + q(1-p)\\
  \end{align*}

  \begin{align*}
    H(Y|X=0) &= p \log \frac 1 p + (1-p) \log \frac 1 {1-p}\\
    H(Y|X=1) &= p \log \frac 1 p + (1-p) \log \frac 1 {1-p}\\
    H(Y|X) &= p \log \frac 1 p + (1-p) \log \frac 1 {1-p}\\
  \end{align*}

  \begin{align*}
    I(X;Y) &= H(Y) - H(Y|X)\\
    &=(p\ast q) \log \frac 1 {p\ast q} + (1-(p\ast q))\log \frac 1 {1-(p\ast q)}
    -
    \left[
    p \log \frac 1 p + (1-p) \log \frac 1 {1-p}
    \right]
  \end{align*}

  We maximize $I(X;Y)$ for $q=1/2$
  \[
    C(W) = \log 2 - h_2(p)
  \]

\end{example}

\begin{example}
  Let $W$ be BEC($p$) and $\Pr{X=1} = q$

  \begin{align*}
    H(X) &= h_2(q)
\\    H(X|Y=0) &= 0\\
    H(X|Y=1) &= 0\\
    H(X|Y=?) &= h_2(q)
  \end{align*}

  \[
    I(X;Y) = h_2(q) - ph_2(q) = (1-p)h_2(q)
  \]
  \[
    C(W) = (1-p)\log 2
  \]
\end{example}

% -----------------------------------
% lesson 10/24/2017 - Sebastien

\subsection{Fano's inequality}
Suppose $U$ and $V$ take values in the same alphabet $\U$, then

\begin{align*}
  H(U | V) \leq p_e \log(|\U| - 1) + h_2(p_e)
\end{align*}
with
\begin{align*}
  p_e = \Pr{U \neq V}
  \quad \text{ and } \quad
  h_2(p) = p \log(\frac{1}{p}) + (1 - p) \log(\frac{1}{(1-p)})
\end{align*}

\begin{proof}
  Define
  \begin{align*}
    Z =
    \left\{
    \begin{array}{ll}
      1 & U \neq V \\
      0 & U = V
    \end{array}
    \right. , \quad
    H(Z) = h_2(p_e)
  \end{align*}

  \begin{align*}
    H(UZ | V) &= H(U|V) + H(Z | UV) \\
    &= H(Z | V) + H(U | VZ) \\
    &\leq H(Z) + H(U | VZ)
  \end{align*}
  but
  \begin{align*}
    H(U | VZ) = \underbrace{H(U | V, Z = 0)}_{0}\Pr{Z = 0} + \underbrace{H(U | V, Z = 1)}_{\leq \log(|\U| - 1)}\underbrace{\Pr{Z = 1}}_{p_e}
  \end{align*}
\end{proof}

So if $H(U|V) > \lambda \Rightarrow \exists f(\lambda) > 0$, $p_e > f(\lambda)$

\begin{corollary}
  Suppose $U^L$, $V^L$ are random sequences with common alphabet $\U$, define :
  \begin{align*}
    p_{e,i} = \Pr{U_i \neq V_i} , \quad
    \bar{p_e} = \frac{1}{L} \sum_{i = 1}^{L} p_{e,i}
  \end{align*}
  then
  \begin{align*}
    \frac{1}{L} H(U^L | V^L) \leq h_2(\bar{p_e}) + \bar{p_e} \log(|\U| - 1)
  \end{align*}
\end{corollary}

\begin{proof}
  \begin{align*}
    \frac{1}{L} H(U^L | V^L) &= \frac{1}{L} \sum_{i = 1}^{L} H(U_i | U^{i-1} V^L) \\
    &\leq \frac{1}{L} \sum_{i = 1}^{L} H(U_i | V_i) \\
    &\leq \frac{1}{L} \sum_{i = 1}^{L}(p_{e,i} \log(|\U| - 1) + h_2(p_{e, i})) \\
    &= \bar{p_e} \log(|\U| - 1) + \frac{1}{L} \sum_{i = 1}^L h_2(p_{e,i}) \\
    &\leq \bar{p_e} \log(|\U| - 1) + h_2(\frac{1}{L} \sum_{i = 1}^L p_{e,i}) \\
    &= \bar{p_e} \log(|\U| - 1) + h_2(\bar{p_e})
  \end{align*}
\end{proof}

\begin{theorem}{"Bad news" theorem, converse to the coding theorem}
  \begin{itemize}
    \item Suppose we have a stationary source $U_1 U_2 ...$ with entropy rate $H$ and produces a letter every $\tau_s$ seconds.
    \item Suppose also that we have a channel $W$ that accepts input $X_1 X_2 ...$ once every $\tau_c$ seconds.
    \item Suppose also
    \begin{align*}
      \frac{H}{\tau_s} > \frac{C(W)}{\tau_c}
    \end{align*}
  \end{itemize}
  then there is a $\lambda > 0$ such that $\bar{p_e} > \lambda$
\end{theorem}

\begin{definition}{stable}
  suppose the encoder works by taking blocks of $L$ letters
  $$(U_1 ... U_L)(U_{L+1} ... U_{2L})...$$
  and outputs
  $$(X_1 ... X_n)(X_{n+1} ... U_{2n})...$$
  then the encoder is stable if
  \begin{align*}
    L \tau_s \geq n \tau_c
  \end{align*}
\end{definition}

\begin{proof}
  Recall that for a stationary source $\frac{H(U_1 ... U_L)}{L}$ tends to $H$ so
  $$ H(U_1 ... U_L) \geq L H$$
  We also have
  \begin{align*}
    I(U^2; V^2) \leq n C(W)
  \end{align*}
  therefore, since $\frac{n}{L} \leq \frac{\tau_s}{\tau_c}$
  \begin{align*}
    H(U^2 | V^2) = \frac{1}{L} (H(U^2) - I(U^2; V^2)) &\geq H - \frac{n}{L} C(W) \\
    &\geq H - \frac{\tau_s}{\tau_c} C(W) \\
    &= \tau_s (\frac{H}{\tau_s} - \frac{C(W)}{\tau_c})
  \end{align*}

  The right hand side is
  $$\epsilon(\tau_c, \tau_s, H, C) > 0$$
  so for every stable encoder, decoder, we have
  \begin{align*}
    \bar{p_e} \log(|\U| - 1) + h_2(\bar{p_e}) > \epsilon(\tau_s, \tau_c, H, C)
  \end{align*}
  then
  \begin{align*}
    \bar{p_e} \geq \epsilon(\tau_s, \tau_c, H, C, |\U|)
  \end{align*}
\end{proof}

\begin{example}
  Suppose $\U = \{ 0, 1 \}$ and $U_1 U_2 ...$ is a Markov process with
    \begin{align*}
    U_1 =
    \left\{
    \begin{array}{ll}
      0 & \text{ with } p = 0.5 \\
      1 & \text{ with } p = 0.5
    \end{array}
    \right. , \quad
    p(U_{n+1} | U_n) =
    \left\{
    \begin{array}{ll}
      1 - p & u_{n+1} = u_n \\
      p     & u_{n+1} \neq u_n
    \end{array}
    \right. , \quad
  \end{align*}
\end{example}

\begin{align*}
  H &= \lim_{n \to \infty} H(U_n | U^{n-1})\\
    &= \lim_{n \to \infty} H(U_n | U_{n_1})\\
    &= H(U_2 | U_1) = h_2(p)
\end{align*}

suppose $w = BEC(q), c(w) = (1 - q)\log(2)$ and $\tau_s = \tau_c = 1$

\begin{align*}
  h_2(\bar{p_e}) \geq h_2(p) - (1 - q)\log(2) \Rightarrow \bar{p_e} \geq \lambda
\end{align*}

What we want to do next is to show a matching "Good news" theorem:

We could show that if $\frac{H}{\tau_s} \leq \frac{c(w)}{\tau_c}$ then for any $\lambda > 0$, we can find a stable encoder and decoder such that $p_e < \lambda$.
Instead, we will show stronger results:
\begin{enumerate}
  \item \textbf{Separation theorem}
    The encoder can be designed in a modular way:
    \begin{itemize}
      \item A \textbf{source encoder} which encoder message words in bits. The design of this encoder is strongly dependent of the type of the input.
      \item A \textbf{channel encoder} which encoder the bits to maximize the performance with a specific channel.
    \end{itemize}
  \item We will show that
  \begin{align*}
    \Pr{U^L \neq V^L} < \lambda
  \end{align*}
  using the fact that
  \begin{align*}
    (U_i \neq V_i) \Rightarrow (U^L \neq V^L)
    \quad \text{ so } \quad
    p_{e,i} \leq \Pr{U^L \neq V^L} \Rightarrow \bar{p_e} \leq \Pr{U^L \neq V^L}
  \end{align*}
\end{enumerate}

We will now show that good channel encoders and channel decoders exist

\begin{definition}
  Given a channel $W$ with input alphabet $\X$, a block encoder is a function
  \begin{align*}
    Enc: \{1, ..., M\} \rightarrow \X^n
  \end{align*}
  with $n$ the block length.\\
  $Enc(1), ..., Enc(M)$ are each called codewords and $M$ is equal to the number of codewords.\\
  The rate of the code can be defined by
  $$ R = \frac{\log M}{n} $$
\end{definition}
\begin{definition}
  Given a channel $W$ with ouput alphabet $Y$, a block decoder is a function
  \begin{align*}
    Dec: \Y^n \rightarrow \{ ?, 1, ..., M\}
  \end{align*}
\end{definition}

\begin{definition}
  \begin{align*}
    p_{error}(m) = \Pr{\hat{m} \neq m | m}
  \end{align*}
  \begin{align*}
    \bar{p}_{error}(m) = \frac{1}{M} \sum_{m=1}^M p_{error}(m)
  \end{align*}
  \begin{align*}
    \hat{p}_{error}(m) = \max_{m} p_{error}(m)
  \end{align*}
\end{definition}


% -----------------------------------
% lesson 11/1/2017 - Sebastien

\subsection{Computational consideration for $C(W)$}
We have an optimization problem
  \begin{align*}
    \max_{p_X} f(p_X) \quad \text{ where } \quad f(p_X) = I(X ; Y)
  \end{align*}
See \cref{sec:appendix-convex} for further information on convex optimization.
  \begin{claim}
    $f$ is a concave function
  \end{claim}




% -----------------------------------
% lesson 2017-11-06 - JB


We want to compute
\[
  \frac {\partial  I(X;Y)} {\partial p (x)}
\]
We have
\[
  I(X;Y) = \sum_{x,y} p(x) W(y|x) \log \frac {W(y|x)} {p_Y(y)}
\]

\[
  p_Y(y) = \sum_x p(x) W(y|x)
\]

\todo{not sure 2nd line}
\begin{align*}
  \frac {\partial I} {\partial p(x_0)}
  &= \sum_{x,y} \frac \partial {\partial p(x_0)}
  \left\{ p(x) W(y|x) \log \frac {W(y|x)} {p_Y(y)} \right\}\\
  &= \sum_{x,y} \left\{ I_{x=x_0} W(y|x)  \log \frac {W(y|x)} {P_Y(y)} -
    p(x)W(y|x) \frac {W(y|x_0)}{p_Y(y)} \log e \right\}\\
  &= \sum_y W(y|x_0) \log \frac {W(y|x_0)}{p_Y(y)} - \sum_y p_Y(y) \frac {W(y|x_0)}{p_Y(y)} \log e\\
  &= \sum_y W(y|x_0) \log \frac {W(y|x)} {P_Y(y)} - \log e
\end{align*}

\begin{theorem}
  \label{th:1000}
  $p_X$ maximizes $I(X;Y)$ iff there exists $\lambda$ such that for all $x$
  \[
    \sum_y W(y|x) \log \frac {W(y|x)} {P_Y(y)} \leq \lambda
  \]
  with equality when $p_X(x) = 0$. Furthermore $\lambda = C(W)$.
\end{theorem}

\begin{proof}
  We only need to prove the furthermore part. Observe that for all $x$
  \[
    p_X(x) \sum_y W(y|x) \log \frac {W(y|x)} {P_Y(y)} =  p_X(x) \lambda
  \]
  and then
  \[
    \sum_x p_X(x) \sum_y W(y|x) \log \frac {W(y|x)} {P_Y(y)} =  \sum_x p_X(x) \lambda
  \]
\end{proof}

\begin{example}[Z channel]
  $W$ is a normal binary channel that maps a 1 input to a 0 output with probability $\epsilon$. Applying \cref{th:1000} with $x=0$ and $x=1$:

  \begin{align*}
    &W(0|0)\log \frac {W(0|0)} {p_Y(0)} = W(0|1)\log \frac {W(0|1)} {p_Y(1)} + W(1|1)\log \frac {W(1|1)} {p_Y(1)}\\
    \iff& \log \frac 1 {p_Y(y)} = \epsilon \log \frac \epsilon {p_Y(0)} + (1-\epsilon) \log \frac {1-\epsilon} {p_Y(1)} = h_2(\epsilon) + \epsilon \log \frac 1 {p_Y(0)} + (1-\epsilon)\log\frac 1 {p_Y(1)}\\
    \iff&\log \frac {p_Y(1)} {p_Y(0)} = - \frac {h_2(\epsilon)}{1-\epsilon} \triangleq -\alpha\\
    \Longrightarrow~&p_Y(1) = \frac {2^{-\alpha}} { 1+2^{-\alpha}} ~\text{and}~p_Y(0) = \frac 1 {1+2^{-\alpha}}
  \end{align*}

  \[
    C(W) = \log (1+2^{-\alpha})
  \]
\end{example}


\begin{lemma}
  For any circle with red segments of cumulative length strictly less than $1/4$, there exists a square whose all corners are on the circle but not on the red segments.
\end{lemma}

\begin{proof}
  By random construction. Place the first corner of the square uniformly at random on the circle (also makes the 3 other uniform).
  \begin{align*}
    \Pr{\text{1st corner lands on red}} &< \frac 1 4\\
    \Pr{i\text{th corner lands on red}} &< \frac 1 4\\
    \Pr{\bigcup\limits_{i=1} i\text{th corner lands on red}} &< 1\\
    \Pr{\text{none of the corners land on red}} &> 0\\
  \end{align*}
\end{proof}

\begin{theorem}[Channel coding - good news]
  Given a channel $W$ (discrete, memoryless, stationary), a rate $R < C(W)$ and $\epsilon > 0$, there exists a $n$ large enough and encoding/decoding functions $Enc:\{1\dots M\} \to \X^n$ with $M \geq 2^{nR}$ and $Dec: \Y^n \to \{1\dots M\}$ such that for all $m\in\{1\dots M\}$
  \[
    \Pr{Dec(Y^n) \not = m | X^n = Enc(m) } < \epsilon
  \]
\end{theorem}

In other words we can communicate reliably at rate greater or equal to $R$ on channel $W$.

\begin{proof}
  Given $W$ and $R<C(W)$, fix a $p_X$ such that $I(X;Y)>R$. Pick $n$
  large enough (to be determined later) and pick $M' = \lceil 2 2^{nR} \rceil$. Define the
  encoding function
  \begin{align*}
    Enc(1) =~&X(1)_1 \dots  X(1)_n\\
    \dots~=~&\dots\\
    Enc(M') =~&X(M')_1 \dots X(M')_n
  \end{align*}
  choosing $\{X(n)_i : 1 \leq i \leq n, 1 \leq m \leq M' \}$ i.i.d. $\sim p_X$.

  For the decoder fix
  \[
    T(n, \delta, p_{XY})= \left\{(x^n, y^n): (1-\delta)p_{XY}(x,y) \leq \frac {\# \{(x_i, y_i) = (x,y)\}} n \leq (1+\delta)p_{XY}(x,y)\right\}
  \]

  $Dec(y^n)$: check for each $m$ if $(Enc(m), y^n) \in T(n,\delta, p_{XY})$, if there is
  only a single $m$ for which the pair is in the typical set then $Dec(y^n) = m$ otherwise
  (if there is none or more than one) $Dec(y^n) = 0$.

  We now compute the probability of error $p_{e,m} \triangleq \Pr{Dec(Y^n) \not = m | X^n = Enc(m)}$. $p_{e,m}$ depends on the choice of $Enc(1) \dots Enc(M)$ and since $Enc(1) \dots Enc(M)$ are randomly chosen, $p_{e,m}$ is a random variable.

  \begin{align*}
    \Ex{p_{e,m}} = \dots
  \end{align*}
  \todo{next week}

\end{proof}

\begin{example}
  Suppose $\X = \{a,b,c\}$, $C(W)=1.3$ and $R=1.25$, then $Enc(1 \dots 32) \to \X^4$ is a valid encoding function for this channel, while $Enc(1 \dots 32) \to \X^5$ would not allow reliable transmission.
\end{example}






% -------------------------------------------------------------------------
% appendices


\newpage
\begin{appendices}
\section{Markov chains}
\label{appendix:markov-chains}

$U_1 - U_2 - \dots - U_n$ forms a Markov chain if the joint probability
distribution of the RVs is
\[
  p(a,b,c,d) = p(a)p(b|a)p(c|b)p(d|c)
\]
which is equivalent to $(U_1, \dots, U_{k-1})$ are independant of $(U_{k+1}, \dots, U_n)$ when conditionned on $U_k$ for any $k$.


\begin{theorem}
  The reverse of a MC is a MC
\end{theorem}


\section{Stochastic processes}
\label{appendix:stoch-proc}

A stochastic process is a collection $U_1, U_2 \dots U_n$ of RVs each taking values in $\U$. It is described by its joint probability
\[
  p(u^n) = P(U_1 \dots U_n = u_1 \dots u_n) = P(U^n = u^n)
\]

\begin{definition}[Stationary stochastic process]
  A process $U_1, U_2, \dots$ is called stationary if for every $n$ and $k$ and $u_1 \dots u_n$, we have
  \[
    p(u^n) = p(U_1 \dots U_n = u_1 \dots u_n) = p(U_{1+k} \dots U_{n+k} = u_1 \dots u_n)
  \]
  In other words, the process is time shift invariant.
\end{definition}

\section{Concave/convex functions}
\label{sec:appendix-convex}

A function $f : S \rightarrow \R $ is called convex if
\[ \forall x,y \in S, 0 \leq \lambda \leq 1,  f(\lambda x - (1 - \lambda)y) \leq \lambda f(x) + (1 - \lambda) f(y) \]
where $S$ is a convex set.

\begin{definition}
  A set $S \subseteq \R^k$ is called to be convex if
  \[ \forall x,y \in S, 0 \leq \lambda \leq 1,  \lambda x + (1 - \lambda) y \in S\]
\end{definition}

\begin{definition}
  $f$ is called concave if $-f$ is convex.
\end{definition}

\begin{definition}{k-simplex}
\[ S_k = \{(p_1, ..., p_k) \in \R^k , p \geq 0, \sum_i p_i = 1 \} \]
as the k-simplex ( a $(k-1)$-dimentional subset of $\R^k$)
\end{definition}

Remark:
Given $S_k$ a convex set and $p,q \in S_k$, let
\begin{align*}
  r   &= \lambda p + (1 - \lambda)q \\
  r_i &= \lambda p_i + (1 - \lambda) q_i \geq 0
\end{align*}

\[ \sum r_i = \lambda + (1 - \lambda) = 1 \]

\begin{example}
  Let $f : S_k \rightarrow \R $, with
  \[ f(p_1, ..., p_k) = \sum^k p_i \log_{\frac{1}{p_i}} \]
  claim: f is concave

  \begin{proof}
    Given $p, q \in S_k, 0 \leq \lambda \leq 1$, define $(U, V)$ with $U \in \{0, 1\}$ and $V \in \{1, ..., k\}$

    \begin{align*}
      P_{UV}(u, v) =
      \left\{
      \begin{array}{l l}
        \lambda p_i, & u=0, v=i \\
        (1 - \lambda)q_i, & u=1, v=i
      \end{array}
      \right.
    \end{align*}
    therefore we have
    \begin{align*}
      \Pr{V = i} &= \lambda p_i + (1 - \lambda) q_i \\
      H(V) &= f(\lambda p + (1 - \lambda) q) \\
      H(V | U) &= \lambda f(p) + (1 - \lambda) f(q) \\
    \end{align*}
  \end{proof}
\end{example}


\begin{example}
  For $W(Y | X)$ let $f(p_X) = I(X ; Y)$ when $p(x, y) = p_X(x) W(Y | X) $
  \textbf{Claim:} $f$ is concave,
  \[ I(X ; Y) = H(Y) - H(X | Y) \]
  and
  \[ H(Y|X) = \sum_x p_X(x) \sum_y W(Y|X) \log \frac{1}{W(Y | X)} \]
  We see that $H(Y|X)$ is a linear function of $p_X(x)$.\\
  $H(Y)$ is a concave function of $p_Y(y)$ with
  \[ p_Y(y) - \sum_x p_X(x) W(Y|X) \]

  \begin{align*}
    p_X \underbrace{\longrightarrow}_{\text{linear}} p_Y \underbrace{\longrightarrow}_{\text{concave}} H(Y)
    \Longrightarrow p_X \underbrace{\longrightarrow}_{\text{concave}} H(Y)
  \end{align*}
\end{example}

How to maximize a function on the simplex?

\begin{theorem}{Karush-Kuhn-Tucker conditions - (KKT)} \\
  Suppose $f: S_k \rightarrow \R$, smooth ($\frac{df}{dp_i dp_j}$ exists), then if $p = \{ p_1, ..., p_k\}$ maximizes $f$, then $\exists \lambda$ s.t.
  \[ \forall i, \frac{df}{d p_i} \leq \lambda \]
  with equality $\forall i$ for which $p_i > 0$

\begin{proof}
  Suppose $(p_1, ..., p_k)$ maximizes $f$, then suppose that $p_i > 0$. Then we can consider a $p' \in S_k$ as follow:\\
  Pick $j \neq i$ and a small $\epsilon, 0 < \epsilon < p_i$
  \begin{align*}
    p_k' =
    \left\{
    \begin{array}{ll}
      p_i - \epsilon, &k = i \\
      p_j + \epsilon, & k = j \\
      p_k, &\text{else}
    \end{array}
    \right.
  \end{align*}

  \begin{align*}
    f(p') &= f(p) + \frac{d f(p)}{d p_i}(- \epsilon) + \frac{d f(p)}{d p_j} (\epsilon) + O(\epsilon^2) \\
    &= f(p) + \epsilon \left[ \frac{df}{dp_j} - \frac{df}{dp_i} \right] + O(\epsilon^2)
  \end{align*}

  So for every $i, j$ with $p_i > 0$ we have
  \[ \frac{df}{dp_j} \frac{df}{dp_i} \]
  $\Rightarrow$ equality if $i$ and $j$ are such that $p_i > 0, p_j > 0$

  $\Rightarrow$ for $i$'s such that $p_i > 0, \frac{df}{dp_i} = \lambda$ and all the indices $j$ have $\frac{df}{dp_j} \leq \lambda$
\end{proof}

\end{theorem}

\begin{theorem}
  Suppose $f: S_k \rightarrow \R$, suppose $f$ is concave and suppose for $p \in S_k$, the KKT condition hold. Then $\forall q \in S_k, f(q) \leq f(p) $

  \begin{proof}
    \begin{align*}
      f(\epsilon q + (1 - \epsilon)p) \geq (1 - \epsilon) f(p) + \epsilon f(q) \\
      \frac{f(\epsilon q + (1 - \epsilon)p) - f(p)}{\lambda} \geq f(q) - f(p), \quad \forall 0 < \epsilon \leq 1
    \end{align*}
    \begin{align*}
      \Rightarrow f(q) - f(p) \leq \lim_{\epsilon \to 0} \frac{f(p + \epsilon(q - p)) - f(p)}{\epsilon}
    \end{align*}
    \begin{align*}
      f(p + \epsilon (q - p)) = f(p) + \sum \epsilon(q_i - p_i) \frac{df(p)}{dp_i} + O(\epsilon^2) \\
      \frac{f(p + \epsilon(q - p)) - f(p)}{\epsilon} = \sum_{i} (q_i - p_i) \frac{d f(p)}{d p_i} + O(\epsilon)
    \end{align*}
    So
    \begin{align*}
      \lim_{\epsilon \to 0} \frac{f(p + \epsilon(q - p)) - f(p)}{\epsilon} = \sum_i (q_i - p_i) \frac{d f(p)}{d p_i}
    \end{align*}
    with
    \begin{align*}
      (q_i - p_i) \frac{df}{dp_i} =
      \left\{
      \begin{array}{ll}
        \lambda (q_i - p_i), & p_i > 0 \\
        \underbrace{(q_i - p_i)}_{\geq 0} \underbrace{\frac{df}{dp_i}}_{\leq \lambda}, & p_i = 0
      \end{array}
      \right.
      \leq \lambda (q_i - p_i)
    \end{align*}
    \begin{align*}
      \Rightarrow f(q) - f(p) \leq \lim_{\epsilon \to 0} [...] \leq 0
    \end{align*}
  \end{proof}
\end{theorem}

\begin{example}
  Supppose $f(p_1, p_2, p_3) = p_1 p_2^2 p_3^3$.
  We want to maximize it. If it isn't concave, we know that $\log(f(..))$ is concave. A try with KKT:
  \begin{align*}
    \frac{df}{dp_1} = \frac{1}{p_1}, \frac{df}{dp_2} = \frac{2}{p_2}, \frac{df}{dp_3} = \frac{3}{p_3}
  \end{align*}
  setting then all $\lambda$ yeild
  \[ (p_1, p_2, p_3) = \lambda (1, 2, 3) = ( \frac{1}{6}, \frac{2}{6}, \frac{3}{6}) \]
\end{example}

\begin{example}
  \[f(p_1, p_2, p_3) = (1 + p_1) p_2 p_3 \]
  maximize $f$ on the simplex by considering
  \[ \log(f) = \log(1 + p_1) + \log(p_2) + \log(p_3) \]
  therefore:
  \[
    \frac{df}{dp_1} = \frac{1}{1 + p_1}, \frac{df}{dp_2} = \frac{1}{p_2}, \frac{df}{dp_3} = \frac{1}{p_3}
  \]
  suggest $p = (0, 0.5, 0.5)$ the $\frac{df}{dp} = (1, 2, 2)$
  $\rightarrow$ satisfy KKT with $\lambda = 2$
\end{example}
\end{appendices}

\end{document}
